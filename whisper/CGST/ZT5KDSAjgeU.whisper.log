
[00:00:00.000 --> 00:00:07.600]  (音樂)
[00:00:07.600 --> 00:00:16.640]  大家好,多謝Felix和院長帶我們分別從哲學和神學的角度
[00:00:16.640 --> 00:00:20.960]  去認識探討AI的本質、發展
[00:00:20.960 --> 00:00:24.960]  以至上帝、人和科技之間的關係
[00:00:24.960 --> 00:00:33.440]  如果他們的說法是宏觀地說到人類的生存、生命的問題
[00:00:33.440 --> 00:00:38.080]  來到這部分,我想把鏡頭拉近一點
[00:00:38.080 --> 00:00:41.440]  或者我們在這一刻最關心、最擔心的
[00:00:41.440 --> 00:00:45.600]  都是開始時說的生計、生活的問題
[00:00:45.600 --> 00:00:50.640]  開始時,Felix帶我們看了一些高危的行業
[00:00:50.640 --> 00:00:57.600]  如果很概括地想,那些工作如果是重複性高
[00:00:57.600 --> 00:01:02.880]  那些專業是處理資料、處理一些規則的時候
[00:01:02.880 --> 00:01:07.760]  或者是一些前線的工作很容易被自動化
[00:01:07.760 --> 00:01:11.120]  可能那些是相對高危的工作
[00:01:11.120 --> 00:01:17.040]  不過過去這大半年,如果是一年前
[00:01:17.040 --> 00:01:20.880]  我們會覺得創意工業是很安全的
[00:01:20.880 --> 00:01:25.120]  科技怎能做到人類的創意呢?
[00:01:25.120 --> 00:01:27.520]  不過大家看到,過去大半年
[00:01:27.520 --> 00:01:32.960]  文字、圖像、影像的工作是首當其衝
[00:01:32.960 --> 00:01:36.800]  反過來,我們不要聽完到現在
[00:01:36.800 --> 00:01:39.840]  大家不知道有沒有覺得「嗯,怎樣啊?」
[00:01:39.840 --> 00:01:42.240]  我們正面一點反過來,問一下AI
[00:01:42.240 --> 00:01:48.400]  其實有什麼工作沒那麼容易被人工智能所取代呢?
[00:01:48.400 --> 00:01:51.520]  我就問Google,不要經常問GPT
[00:01:51.520 --> 00:01:54.800]  它就給了我這十個
[00:01:54.800 --> 00:02:00.320]  這十個,不知道大家有沒有成為其中一份子
[00:02:00.320 --> 00:02:03.440]  如果我們又很概括地看
[00:02:03.440 --> 00:02:08.160]  這些職業對人的身心靈的需要
[00:02:08.160 --> 00:02:14.320]  它會作出一些支援、治療、發展
[00:02:14.320 --> 00:02:19.680]  滿足人類對那些美、對愛心、對關係的追求
[00:02:19.680 --> 00:02:25.040]  這一類的職業,或者暫時來說都是安全的
[00:02:25.040 --> 00:02:27.520]  除了工作被取代之外
[00:02:27.520 --> 00:02:32.240]  殺到埋身的AI的倫理問題又何止那麼少
[00:02:32.240 --> 00:02:37.440]  你每一天聽新聞,其實都有很多關於AI的討論
[00:02:37.440 --> 00:02:40.640]  剛才提到的自動化的決策系統
[00:02:40.640 --> 00:02:42.640]  是不是真的那麼成熟安全呢?
[00:02:42.640 --> 00:02:45.040]  自動駕駛已經搞了很久了
[00:02:45.040 --> 00:02:51.840]  還不至於很安全地去搭這些車
[00:02:51.840 --> 00:02:55.440]  演算法背後是不是有黑箱作業
[00:02:55.440 --> 00:02:57.920]  或者是用什麼數據來做
[00:02:57.920 --> 00:02:59.920]  這些數據會不會有偏見呢?
[00:02:59.920 --> 00:03:05.040]  大家最擔心或者最附身的是假新聞、假資訊
[00:03:05.040 --> 00:03:06.560]  其實越來越難分辨
[00:03:06.560 --> 00:03:11.840]  最近大半年大家應該每天都收到兩至三個不知從何而來的電話
[00:03:11.840 --> 00:03:15.200]  假新聞的技術越來越成熟
[00:03:15.200 --> 00:03:19.040]  令到不單止人類可以讓它成為工具
[00:03:19.040 --> 00:03:21.040]  更加成為騙徒的工具
[00:03:21.040 --> 00:03:24.800]  版權問題、私隱問題、監控問題
[00:03:24.800 --> 00:03:28.960]  剛才提到的這些問題好像比答案更加多
[00:03:28.960 --> 00:03:31.920]  劇情其實很熟悉
[00:03:31.920 --> 00:03:36.320]  我們回顧過去幾十年的科技短短的發展歷史
[00:03:36.320 --> 00:03:38.320]  其實我們看到一個模式
[00:03:38.320 --> 00:03:41.440]  每逢一個新的浪潮的科技出現的時候
[00:03:41.440 --> 00:03:44.800]  首先會帶來一陣的亢奮和憂慮
[00:03:44.800 --> 00:03:48.160]  過度的亢奮和過度的憂慮
[00:03:48.160 --> 00:03:52.560]  其實是源自於相信科技能夠決定一切
[00:03:52.560 --> 00:03:54.560]  決定人類的未來
[00:03:54.560 --> 00:03:59.520]  一種科技決定論
[00:03:59.520 --> 00:04:02.000]  無論你帶著這個決定論
[00:04:02.000 --> 00:04:08.080]  是覺得很樂觀地認為科技能夠解決人類生活的問題
[00:04:08.080 --> 00:04:10.080]  或者很悲觀地覺得
[00:04:10.080 --> 00:04:13.200]  沒有了,AI始終都會取代人類
[00:04:13.200 --> 00:04:19.360]  兩者其實都會認為科技的發展彷彿神一般的存在
[00:04:19.360 --> 00:04:23.200]  在主宰人類的未來
[00:04:23.200 --> 00:04:29.440]  不過我們站在最前線經歷了這幾十年的科技的人
[00:04:29.440 --> 00:04:31.440]  去回顧這些歷史的時候
[00:04:31.440 --> 00:04:34.560]  我們發現科技發展不是一條線
[00:04:34.560 --> 00:04:37.440]  不是一種決定性的力量
[00:04:37.440 --> 00:04:40.400]  每逢有新的事情發展的時候
[00:04:40.400 --> 00:04:43.840]  過度的亢奮和過度的憂慮之後
[00:04:43.840 --> 00:04:48.320]  社會會進入一種初商的階段
[00:04:48.320 --> 00:04:52.480]  各種的聲音就會被引入討論
[00:04:52.480 --> 00:04:54.480]  今晚都是其中一個例子
[00:04:54.480 --> 00:05:01.120]  AI的可能性、發展的政策、人類的權益、潛在的危機
[00:05:01.120 --> 00:05:03.680]  各種即時的害處和悲憶
[00:05:03.680 --> 00:05:08.480]  有沒有濫用技術的防止方法等等
[00:05:08.480 --> 00:05:10.480]  對自然生態的影響等等
[00:05:10.480 --> 00:05:12.480]  很多人都會加入這個討論
[00:05:12.480 --> 00:05:15.760]  初商和國力的過程裡面
[00:05:15.760 --> 00:05:19.120]  社會其實未必一定會達到一定的共識
[00:05:19.120 --> 00:05:21.120]  或者只有一個單一的答案
[00:05:21.120 --> 00:05:25.600]  不過在討論初商國力的過程裡面
[00:05:25.600 --> 00:05:30.400]  我們會塑造人類和科技如何來共存
[00:05:30.400 --> 00:05:33.280]  正如剛才Felix所說的共生的關係
[00:05:33.280 --> 00:05:37.440]  社會學家就會覺得是一種科技的社會形素
[00:05:37.440 --> 00:05:41.280]  一個社會形式的建構過程
[00:05:41.280 --> 00:05:43.280]  如果我們從這個角度去看的話
[00:05:43.280 --> 00:05:47.200]  我們就要問教會、信仰群體
[00:05:47.200 --> 00:05:52.960]  從什麼角度來參與建構初商的過程
[00:05:52.960 --> 00:05:56.000]  我們能不能夠化被動為主動
[00:05:56.000 --> 00:06:01.040]  讓我們對AI的憂慮轉化成為一種塑造的力量
[00:06:01.040 --> 00:06:05.600]  參與建設AI成為一個有道德的產物
[00:06:05.600 --> 00:06:10.080]  或者在發展的過程和成品上有倫理的基礎
[00:06:10.080 --> 00:06:13.600]  讓AI再次置放在剛才院長所說的
[00:06:13.600 --> 00:06:17.600]  上帝創造心意裡面一個合適的位置
[00:06:17.600 --> 00:06:20.160]  說到AI有道德
[00:06:20.160 --> 00:06:25.440]  其實信任的AI、道德的AI、責任的AI
[00:06:25.440 --> 00:06:29.920]  這些名稱並不是新鮮的事物
[00:06:29.920 --> 00:06:34.880]  歐盟早幾年已經列出了一系列的原則
[00:06:34.880 --> 00:06:37.680]  說什麼是可信的AI
[00:06:37.680 --> 00:06:41.280]  或者我們今天也可以從這個角度出發
[00:06:41.280 --> 00:06:45.920]  看看信仰群體能夠帶來什麼樣的思考
[00:06:45.920 --> 00:06:51.600]  其實香港政府也有類似的人工智能道德框架
[00:06:51.600 --> 00:06:53.280]  不過是大同小異
[00:06:53.280 --> 00:06:55.520]  這裡列出的七項原則
[00:06:55.520 --> 00:07:00.560]  其實每一項都可以用來做一個單獨的探討和反思
[00:07:00.560 --> 00:07:02.560]  不過今天時間所限
[00:07:02.560 --> 00:07:06.560]  我特別想說第一項和第七項
[00:07:06.560 --> 00:07:12.080]  對於無論你今天是在做AI或者在用AI
[00:07:12.080 --> 00:07:15.040]  可以作為一個思考的起點
[00:07:15.040 --> 00:07:19.120]  人類自主和監督
[00:07:20.000 --> 00:07:26.480]  這個原則大概是說AI要尊重人的自主權和決策
[00:07:26.480 --> 00:07:32.160]  讓每個人能夠對AI的系統具有最終的控制權
[00:07:32.160 --> 00:07:37.360]  並且要提供一個透明和可以理解的輸出
[00:07:37.360 --> 00:07:43.120]  無論AI的能力發展到何等強大的地步
[00:07:43.120 --> 00:07:46.800]  其實人類的確要堅持一個道德主體
[00:07:46.800 --> 00:07:49.840]  這個不可以替代的角色
[00:07:49.840 --> 00:07:53.520]  其實近來對AI發展的討論
[00:07:53.520 --> 00:07:59.520]  大部分的討論都認為人類自主和監督是很重要的
[00:07:59.520 --> 00:08:06.160]  Human in the loop這種AI和人類一起工作的發展和應用
[00:08:06.160 --> 00:08:08.880]  基本上是一個共識
[00:08:08.880 --> 00:08:11.440]  不過我們要再問下去
[00:08:11.440 --> 00:08:17.200]  這個所謂人類自主的Human agency是一個怎樣的自主呢?
[00:08:17.200 --> 00:08:22.560]  這裡所說的人類是否所有人類都有權自主呢?
[00:08:22.560 --> 00:08:25.200]  還是只是某一部分的人呢?
[00:08:25.200 --> 00:08:29.520]  是否所有用戶都有最大程度的自主
[00:08:29.520 --> 00:08:33.520]  還是只是科技企業的管理層呢?
[00:08:33.520 --> 00:08:40.320]  剛才Elon Musk他們簽了一個宣言
[00:08:40.320 --> 00:08:45.680]  後來Elon Musk自己就決定發展自己的AI
[00:08:45.680 --> 00:08:51.520]  因為他覺得自己的AI才可以阻止人類的滅亡
[00:08:51.520 --> 00:08:55.680]  他們是一個這樣的想法
[00:08:55.680 --> 00:08:58.960]  還是政府監管的人呢?
[00:08:58.960 --> 00:09:04.240]  還是最後都落在程式人手上呢?
[00:09:04.240 --> 00:09:08.240]  事實上這些人所擁抱的價值觀
[00:09:08.240 --> 00:09:14.320]  是會決定他們發展AI的項目的潛藏邏輯
[00:09:14.320 --> 00:09:15.920]  AI怎樣去學習
[00:09:15.920 --> 00:09:17.920]  用什麼數據去學習
[00:09:17.920 --> 00:09:19.600]  學習什麼東西
[00:09:19.600 --> 00:09:21.120]  限制什麼東西
[00:09:21.120 --> 00:09:22.800]  輸出什麼東西
[00:09:22.800 --> 00:09:26.320]  其實都是以這部分人的價值觀去塑造
[00:09:26.320 --> 00:09:28.320]  其實你用開ChartGP你就知道
[00:09:28.320 --> 00:09:30.320]  有些東西是不會回答你的
[00:09:30.320 --> 00:09:33.600]  但誰決定這些東西是不會回答的呢?
[00:09:33.600 --> 00:09:36.880]  所以有很多權力的爭論其實在其中
[00:09:36.880 --> 00:09:38.880]  對於我們今天來說
[00:09:38.880 --> 00:09:44.960]  與其去問AI的權力應該歸屬什麼人
[00:09:44.960 --> 00:09:46.000]  什麼角色
[00:09:46.000 --> 00:09:48.000]  倒不如我們去問
[00:09:48.000 --> 00:09:51.520]  要自主和監督AI的人類
[00:09:51.520 --> 00:09:54.720]  其實應該擁有什麼生命的質素
[00:09:54.720 --> 00:09:57.840]  擁有什麼品格和能力呢?
[00:09:57.840 --> 00:10:01.680]  如果將我們的焦點放在品格質素
[00:10:01.680 --> 00:10:03.680]  而不是角色的權力
[00:10:03.680 --> 00:10:06.560]  那麼基督教會就有很多東西可以說了
[00:10:06.560 --> 00:10:12.640]  畢竟很多基督徒每天散落在不同職場的崗位上
[00:10:12.640 --> 00:10:16.960]  為主、為人類、為世界去盡忠、去工作
[00:10:16.960 --> 00:10:20.880]  當中不缺乏很多的企業家、公務員
[00:10:20.880 --> 00:10:22.400]  甚至是程式人
[00:10:22.400 --> 00:10:24.400]  甚至是一般的使用者
[00:10:24.400 --> 00:10:30.400]  剛才院長提到發展科技要操練敬虔和美德
[00:10:30.400 --> 00:10:33.600]  比起發展技術更加重要
[00:10:33.600 --> 00:10:37.680]  面對AI人要去自主自由
[00:10:37.680 --> 00:10:42.800]  想去駕馭科技而不是被科技操控
[00:10:42.800 --> 00:10:47.040]  我就引用後世的一句說話
[00:10:47.040 --> 00:10:50.960]  如果看自由不是一種狀態、一個status
[00:10:50.960 --> 00:10:52.960]  而是一種技巧
[00:10:52.960 --> 00:10:58.640]  從這個角度來看有德性的人才能得到自由
[00:10:58.640 --> 00:11:05.360]  如果將人類的自由看成一種技巧、一種能力
[00:11:05.360 --> 00:11:07.360]  而不是一種狀態
[00:11:07.360 --> 00:11:11.200]  我們很自然地將關注點不是爭取status
[00:11:11.200 --> 00:11:19.120]  而是我們怎樣訓練自己成為有技巧、一種有德性的發展
[00:11:19.120 --> 00:11:22.160]  這種技巧不是一種技術技巧
[00:11:22.160 --> 00:11:26.160]  不是叫大家上AI課或讀程式學位
[00:11:26.160 --> 00:11:28.480]  而是一種德性的發展
[00:11:28.480 --> 00:11:31.760]  我用教育界作為一個例子
[00:11:31.760 --> 00:11:37.040]  當ChartGBT出現時教育界很擔心
[00:11:37.040 --> 00:11:44.080]  以往的教學和評核方法在AI新世界不再可行
[00:11:44.080 --> 00:11:48.080]  於是有學校馬上禁止使用
[00:11:48.080 --> 00:11:53.680]  亦有人會去發展AI偵測的技術
[00:11:53.680 --> 00:12:00.080]  嘗試用技術、規管的層面去對應這些問題
[00:12:00.080 --> 00:12:03.600]  不過各種技術走到今天
[00:12:03.600 --> 00:12:07.280]  很多實測都告訴我們其實不是太理想
[00:12:07.280 --> 00:12:11.280]  人如果有心走漏洞、有心走捷徑
[00:12:11.280 --> 00:12:15.120]  現在實在太多方法、太方便
[00:12:15.120 --> 00:12:19.760]  去到最後似乎發展人類的品格教育
[00:12:19.760 --> 00:12:26.480]  培育對AI有自主、自由的品格的能力才是出路
[00:12:26.480 --> 00:12:31.680]  正如侯氏所說,有德性的人才能得到自由
[00:12:31.680 --> 00:12:35.200]  當然今天說到的只是一個起始點
[00:12:35.200 --> 00:12:39.280]  面對這個新世界,這些品格是甚麼來的
[00:12:39.280 --> 00:12:40.720]  如何去培養呢?
[00:12:40.720 --> 00:12:46.000]  我相信這個所謂的起點大家要繼續探討和反省
[00:12:46.000 --> 00:12:52.000]  第二個想提的原則是負責任的問題
[00:12:52.000 --> 00:12:57.920]  這裡提到在AI的開發、部署和使用中所涉及的人
[00:12:57.920 --> 00:13:03.600]  應該對AI系統的結果和影響要去負責任
[00:13:03.600 --> 00:13:07.600]  確保一個透明負責任和擁有一個補救的機制
[00:13:07.600 --> 00:13:08.880]  這個原則
[00:13:08.880 --> 00:13:13.280]  對於我們來說,第二點這個負責任的AI
[00:13:13.280 --> 00:13:16.960]  可以說是第一點的延伸或例子
[00:13:16.960 --> 00:13:23.440]  因為有德性的人自然會為自己的行為的決定去負責任
[00:13:23.440 --> 00:13:26.320]  我們活在一個世代裏面
[00:13:26.320 --> 00:13:33.120]  讓我們覺得有時這些文化很容易令人害怕去作決定、選擇
[00:13:33.120 --> 00:13:35.600]  因為作選擇要面對結果
[00:13:35.600 --> 00:13:39.680]  面對結果的時候就要為這些結果負責任
[00:13:39.680 --> 00:13:46.400]  AI就提供了一個逃避責任的出口和藉口
[00:13:46.400 --> 00:13:51.200]  早前有一個科技公司經濟不要裁員
[00:13:51.200 --> 00:13:54.240]  對外公佈裁了一百多人
[00:13:54.240 --> 00:14:00.880]  他的說法是說我全部是根據AI的判斷去作出
[00:14:00.880 --> 00:14:03.840]  誰可以留下,誰要走
[00:14:03.840 --> 00:14:07.520]  感覺像什麼呢?比拉多,比拉多洗手
[00:14:07.520 --> 00:14:11.840]  不關我的事,這班人要殺耶穌
[00:14:11.840 --> 00:14:14.560]  我只是執行這班人的決定
[00:14:14.560 --> 00:14:17.520]  這個說法當然被人大力批評
[00:14:17.520 --> 00:14:21.760]  AI的決定去解僱這班人是根據什麼原則、數據
[00:14:21.760 --> 00:14:23.520]  有沒有人在參與其中
[00:14:23.520 --> 00:14:27.040]  AI成為了解釋一切關鍵字
[00:14:27.040 --> 00:14:30.240]  當中的運作沒有人知曉
[00:14:30.240 --> 00:14:33.760]  後來有內部的電郵流了出來
[00:14:33.760 --> 00:14:38.640]  原來AI是根據員工的數碼足跡
[00:14:38.640 --> 00:14:43.600]  數碼足跡來判斷一班人的低生產力
[00:14:43.600 --> 00:14:47.120]  數碼足跡大概是指你去過什麼網站
[00:14:47.120 --> 00:14:48.640]  用過什麼APP
[00:14:48.640 --> 00:14:52.480]  去過多久,做過什麼
[00:14:52.480 --> 00:14:58.080]  然後AI就用什麼演算法去推斷員工的低生產力
[00:14:58.080 --> 00:15:03.360]  如果是這樣,無論是被解僱或是留下的人
[00:15:03.360 --> 00:15:06.880]  對於他們有多大的說服力
[00:15:06.880 --> 00:15:10.480]  對於留下後在公司工作的同事
[00:15:10.480 --> 00:15:12.880]  他們會怎樣繼續工作下去
[00:15:12.880 --> 00:15:15.200]  其實是會產生很大的影響
[00:15:15.200 --> 00:15:21.760]  AI參與人類的決定其實不一定是壞事
[00:15:21.760 --> 00:15:27.680]  透過它可能幫到我們去多一些資料、一些角度
[00:15:27.680 --> 00:15:36.640]  但是AI的資料、角度不可以成為人逃避責任的藉口
[00:15:36.640 --> 00:15:43.680]  人也有責任去解釋AI是怎樣運作得出這些結果
[00:15:43.680 --> 00:15:48.160]  當然這些暫時只是一些少數
[00:15:48.160 --> 00:15:49.840]  未成為一個主流
[00:15:49.840 --> 00:15:52.640]  不過也足以叫我們去問
[00:15:52.640 --> 00:15:56.880]  人的責任究竟是否真的可以外判給AI
[00:15:56.880 --> 00:16:01.760]  對於我們基督徒來說,責任不單止是這樣
[00:16:01.760 --> 00:16:07.920]  責任更加是要面對上帝、面對被造的世界
[00:16:07.920 --> 00:16:10.720]  尤其是弱小的群體
[00:16:10.720 --> 00:16:14.240]  在AI新世界的處境裡
[00:16:14.240 --> 00:16:18.080]  弱小不單止是孤寡老弱
[00:16:18.080 --> 00:16:22.960]  也包括了在數碼世代裡被邊緣化
[00:16:22.960 --> 00:16:25.200]  欠缺一些科技的能力
[00:16:25.200 --> 00:16:29.680]  而去面對失業甚至是生活困難的人
[00:16:29.680 --> 00:16:33.280]  我們開發或使用AI的人
[00:16:33.280 --> 00:16:38.160]  必須去問這個AI的應用
[00:16:38.160 --> 00:16:41.200]  對哪些人帶來了負面的影響
[00:16:41.200 --> 00:16:45.200]  怎樣去舒緩、怎樣去幫助、怎樣去補救
[00:16:45.200 --> 00:16:49.280]  怎樣去保護、預防這些負面的影響
[00:16:49.280 --> 00:16:52.400]  我們不單止要看它正面的可能性
[00:16:52.400 --> 00:16:57.040]  要去負責任地讓AI成為弱小的幫助
[00:16:57.040 --> 00:16:59.040]  而不是一些新的障礙
[00:16:59.040 --> 00:17:03.760]  最後我想用一個正面的例子
[00:17:03.760 --> 00:17:09.680]  今天大家聽了很多令人驚嚇的故事
[00:17:09.680 --> 00:17:11.680]  另一個教育界的例子
[00:17:11.680 --> 00:17:14.560]  其實AI可以做到什麼呢?
[00:17:14.560 --> 00:17:18.960]  Khan Academy是美國一個非牟利的機構
[00:17:18.960 --> 00:17:22.960]  設立網上教育資源的平台
[00:17:22.960 --> 00:17:26.960]  它的使命是要向世界任何地方
[00:17:26.960 --> 00:17:30.560]  任何的小朋友都提供一些免費的教育
[00:17:30.560 --> 00:17:33.360]  教育界的朋友都聽過
[00:17:33.360 --> 00:17:37.360]  最新他們有一個項目叫AI Chatbot
[00:17:37.360 --> 00:17:43.440]  目標是讓這個Chatbot成為小朋友的個人導師
[00:17:43.440 --> 00:17:49.040]  畢竟不是世界各地的人都有免費教育
[00:17:49.040 --> 00:17:51.040]  或者是有老師去教他們
[00:17:51.040 --> 00:17:56.880]  一般的網上教育資源也未必可以針對小朋友個別學習的問題
[00:17:56.880 --> 00:17:59.600]  這個Chatbot就好像ChatGPT一樣
[00:17:59.600 --> 00:18:01.600]  你可以問他學習上的問題
[00:18:01.600 --> 00:18:03.600]  不過和ChatGPT不同
[00:18:03.600 --> 00:18:05.600]  他不會直接給你答案
[00:18:05.600 --> 00:18:08.720]  大家可以看到他的一些對答
[00:18:09.040 --> 00:18:15.760]  就像老師一樣,一步一步去引導小朋友思考答案
[00:18:15.760 --> 00:18:21.360]  大部分我們都只是認識某些大品牌的AI Chatbot
[00:18:21.360 --> 00:18:27.840]  人可以打一些提示去操作,操控他得到任何的答案
[00:18:27.840 --> 00:18:30.800]  這個AI Chatbot的設計者
[00:18:30.800 --> 00:18:32.800]  首先去問一個問題
[00:18:32.800 --> 00:18:34.800]  教育的目的是什麼
[00:18:34.880 --> 00:18:40.240]  然後按照目的,按照他專業的認識去設計
[00:18:40.240 --> 00:18:48.400]  去限制,去引導,去將AI塑造成為一個按教育目的而成的輔助工具
[00:18:48.400 --> 00:18:52.880]  教會,包括各行各業的基督徒
[00:18:52.880 --> 00:18:57.520]  無論你是AI的開發者或者是使用者
[00:18:57.520 --> 00:18:59.520]  面對這個新的世界
[00:18:59.520 --> 00:19:03.200]  我們是否可以化被動為主動
[00:19:03.200 --> 00:19:08.320]  讓我們將對AI的憂慮轉化為一些塑造的力量
[00:19:08.320 --> 00:19:13.120]  因為說到首先要成為一個有德性的自由人
[00:19:13.120 --> 00:19:15.120]  有能力去反思
[00:19:15.120 --> 00:19:17.120]  勇於去負責
[00:19:17.120 --> 00:19:19.120]  勇於去改正
[00:19:19.120 --> 00:19:21.120]  為他者,為弱勢而活
[00:19:21.120 --> 00:19:26.240]  讓AI成為有品格的輔助工具
[00:19:26.240 --> 00:19:30.720]  將AI置放在上帝創造心意的適當位置
[00:19:30.720 --> 00:19:37.120]  或者今天的教會,信仰的群體要繼續思考和行動的課題
[00:19:37.840 --> 00:19:49.200]  (音樂)


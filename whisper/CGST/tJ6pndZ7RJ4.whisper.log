
[00:00:00.000 --> 00:00:07.600]  (音樂)
[00:00:07.600 --> 00:00:10.640]  按時間都會問大家的
[00:00:10.640 --> 00:00:13.440]  不過我做主席當然要兇一點
[00:00:13.440 --> 00:00:16.640]  首先問一個問題
[00:00:16.640 --> 00:00:20.080]  先問一下Felix一個很簡單的問題
[00:00:20.080 --> 00:00:23.200]  最近有沒有問checkGPT應該買哪隻股票
[00:00:23.200 --> 00:00:25.200]  沒有
[00:00:25.200 --> 00:00:28.080]  我不是很敢買股票的
[00:00:28.080 --> 00:00:31.680]  我剛才有幅圖
[00:00:31.680 --> 00:00:33.680]  我不知道是誰找回來的
[00:00:33.680 --> 00:00:39.200]  他買股票的能力好像比Ron Buffett好
[00:00:39.200 --> 00:00:41.200]  我不知道是真是假
[00:00:41.200 --> 00:00:43.200]  我不會看內容
[00:00:43.200 --> 00:00:47.760]  問一個比較具體的問題
[00:00:47.760 --> 00:00:51.760]  我發覺很多人都問院長
[00:00:51.760 --> 00:00:56.240]  可能你講了一些很偉大的事情
[00:00:56.240 --> 00:00:58.240]  我先問一下你的偉大事情
[00:00:58.240 --> 00:01:01.360]  你提到AI主要有兩個危機
[00:01:01.360 --> 00:01:03.360]  一個是Singularity
[00:01:03.360 --> 00:01:05.360]  AI聰明到一個地步
[00:01:05.360 --> 00:01:08.240]  可以像Terminator一樣毀滅人類
[00:01:08.240 --> 00:01:10.720]  不過我覺得如果AI去到一個地步
[00:01:10.720 --> 00:01:12.720]  我都沒得救了
[00:01:12.720 --> 00:01:16.080]  我只希望下一個突破是100年之後
[00:01:16.080 --> 00:01:18.080]  不是我一生的了
[00:01:18.080 --> 00:01:20.080]  但你講到另一個問題
[00:01:20.080 --> 00:01:22.640]  Digital Totalitarianism
[00:01:22.640 --> 00:01:25.280]  這個似乎就近很多了
[00:01:25.280 --> 00:01:27.280]  你講到Student
[00:01:27.280 --> 00:01:29.280]  那些都好像是正在發生的
[00:01:29.280 --> 00:01:31.280]  我想問一下作為一個
[00:01:31.280 --> 00:01:33.280]  Educated Citizen
[00:01:33.280 --> 00:01:35.280]  一個有教育的公民
[00:01:35.280 --> 00:01:39.280]  其實在社會裏面
[00:01:39.280 --> 00:01:41.280]  我作為一個公民
[00:01:41.280 --> 00:01:43.280]  有什麼可以做
[00:01:43.280 --> 00:01:45.280]  去面對或者嘗試
[00:01:45.280 --> 00:01:49.280]  等這一天不會來臨呢?
[00:01:49.280 --> 00:01:53.280]  通常我們在
[00:01:53.280 --> 00:01:55.280]  剛才說的Philosophy of Technology
[00:01:55.280 --> 00:02:01.280]  我們對於如何面對科技的操控
[00:02:01.280 --> 00:02:05.280]  有兩個角度去看
[00:02:05.280 --> 00:02:09.280]  第一個就是Spiritual的角度
[00:02:09.280 --> 00:02:11.280]  一個人如何面對的角度
[00:02:11.280 --> 00:02:15.280]  第二個就是科技本身如何去發展的角度
[00:02:15.280 --> 00:02:19.280]  剛才你問過應該是屬於人如何面對的角度
[00:02:19.280 --> 00:02:23.280]  我會想起
[00:02:23.280 --> 00:02:27.280]  我老婆很喜歡看YouTube
[00:02:27.280 --> 00:02:29.280]  她推薦什麼都看
[00:02:29.280 --> 00:02:33.280]  我就會叫她不要別人推薦你就看
[00:02:33.280 --> 00:02:35.280]  你寧願自己搜尋一些你想看的
[00:02:35.280 --> 00:02:37.280]  因為她推薦你的東西
[00:02:37.280 --> 00:02:40.280]  其實她已經在收集你的資料
[00:02:40.280 --> 00:02:42.280]  而操控著你
[00:02:42.280 --> 00:02:47.280]  你沒有理由讓她自由地去操控你
[00:02:47.280 --> 00:02:49.280]  其中一件事就是
[00:02:49.280 --> 00:02:53.280]  當你正在使用科技產品的時候
[00:02:53.280 --> 00:02:56.280]  自己也要在意他們背後在做的事
[00:02:56.280 --> 00:02:59.280]  如果你在意他們背後在做的事的時候
[00:02:59.280 --> 00:03:03.280]  你也會嘗試採用一些方法
[00:03:03.280 --> 00:03:05.280]  令自己不會被他們影響
[00:03:05.280 --> 00:03:09.280]  就像Netflix
[00:03:09.280 --> 00:03:11.280]  我老婆也是
[00:03:11.280 --> 00:03:13.280]  Facebook現在的短片
[00:03:13.280 --> 00:03:15.280]  一個連一個
[00:03:15.280 --> 00:03:17.280]  我老婆就是這樣
[00:03:17.280 --> 00:03:21.280]  不過你這個問題引申我又想繼續問你
[00:03:21.280 --> 00:03:23.280]  也想問一下Alex
[00:03:23.280 --> 00:03:25.280]  Alex剛才提到
[00:03:25.280 --> 00:03:29.280]  所謂做一個Virtuous person去用科技
[00:03:29.280 --> 00:03:31.280]  剛才Felix也舉了一個例子
[00:03:31.280 --> 00:03:34.280]  他說你跟著他去做
[00:03:34.280 --> 00:03:37.280]  可能就不是最Virtuous
[00:03:37.280 --> 00:03:39.280]  起碼不是最聰明的做法
[00:03:39.280 --> 00:03:43.280]  其實有沒有一些比較具體的例子
[00:03:43.280 --> 00:03:46.280]  例如你剛才提到的Develop Education
[00:03:46.280 --> 00:03:47.280]  不是我做的
[00:03:47.280 --> 00:03:48.280]  或者是你做到的
[00:03:48.280 --> 00:03:49.280]  不是我做到的
[00:03:49.280 --> 00:03:52.280]  我們是普通的普通的用戶
[00:03:52.280 --> 00:03:53.280]  怎樣可以做一個
[00:03:53.280 --> 00:03:57.280]  所謂更加Virtuous去用科技的人呢?
[00:03:57.280 --> 00:03:59.280]  Felix或是Alex
[00:03:59.280 --> 00:04:00.280]  補充一下
[00:04:00.280 --> 00:04:02.280]  我經常覺得
[00:04:02.280 --> 00:04:05.280]  你首先要做一個Virtuous的人
[00:04:05.280 --> 00:04:07.280]  在這個世代裏
[00:04:07.280 --> 00:04:09.280]  你首先要讓自己成為
[00:04:09.280 --> 00:04:11.280]  你要知道他發生什麼事
[00:04:11.280 --> 00:04:14.280]  其實一個Literary的人
[00:04:14.280 --> 00:04:17.280]  你要做一個有Virtual的人
[00:04:17.280 --> 00:04:21.280]  首先你要成為一個Literary的人
[00:04:21.280 --> 00:04:25.280]  你要明白背後的運作
[00:04:25.280 --> 00:04:26.280]  或者是認識
[00:04:26.280 --> 00:04:27.280]  例如聽講座
[00:04:27.280 --> 00:04:30.280]  幫助你去知道更多
[00:04:30.280 --> 00:04:33.280]  其實他背後在發生什麼事
[00:04:33.280 --> 00:04:34.280]  當然剛才我說到
[00:04:34.280 --> 00:04:37.280]  未必是要你去上一個課程
[00:04:37.280 --> 00:04:40.280]  但是很多的文章或者是一些反省
[00:04:40.280 --> 00:04:43.280]  一直告訴我們
[00:04:43.280 --> 00:04:46.280]  這些科技背後
[00:04:46.280 --> 00:04:50.280]  其實他們是在用什麼價值觀等等
[00:04:50.280 --> 00:04:54.280]  當你有對科技的認識
[00:04:54.280 --> 00:04:58.280]  你明白它的限制和邏輯的時候
[00:04:58.280 --> 00:05:02.280]  你就會開始慢慢產生一種判斷的能力
[00:05:02.280 --> 00:05:04.280]  一個Determine的能力
[00:05:04.280 --> 00:05:07.280]  所以這是一個過程
[00:05:07.280 --> 00:05:09.280]  或者可以這樣說
[00:05:09.280 --> 00:05:11.280]  這個世代是很喜歡
[00:05:11.280 --> 00:05:13.280]  這個數碼文化
[00:05:13.280 --> 00:05:14.280]  很喜歡Feed東西給你
[00:05:14.280 --> 00:05:15.280]  剛才我們一直說到
[00:05:15.280 --> 00:05:19.280]  用它的方法去Feed一些Data
[00:05:19.280 --> 00:05:20.280]  或者Feed一些內容給你
[00:05:20.280 --> 00:05:23.280]  我們其實是很習慣有一種
[00:05:23.280 --> 00:05:25.280]  一看就知道
[00:05:25.280 --> 00:05:26.280]  那種文化
[00:05:26.280 --> 00:05:27.280]  坦白說,我也要承認
[00:05:27.280 --> 00:05:30.280]  我有時候自己也會很累的時候
[00:05:30.280 --> 00:05:33.280]  你送什麼給我,我就看什麼
[00:05:33.280 --> 00:05:35.280]  那是一個問題
[00:05:35.280 --> 00:05:38.280]  我們要有一種很大的Awareness
[00:05:38.280 --> 00:05:43.280]  Awareness是一個重要的元素
[00:05:43.280 --> 00:05:49.280]  你要面對他們那種Feed給你的東西
[00:05:49.280 --> 00:05:51.280]  你要主動地去抗衡
[00:05:51.280 --> 00:05:53.280]  我暫時就說到這裡
[00:05:53.280 --> 00:05:54.280]  不知道兩位有沒有其他補充
[00:05:54.280 --> 00:05:56.280]  最後一句就補充一下
[00:05:56.280 --> 00:05:59.280]  Virtual或者美德一定是操練
[00:05:59.280 --> 00:06:01.280]  而操練當然是不舒服
[00:06:01.280 --> 00:06:03.280]  就好像你去跑步
[00:06:03.280 --> 00:06:04.280]  當然是不舒服
[00:06:04.280 --> 00:06:06.280]  就是因為科技或者AI
[00:06:06.280 --> 00:06:07.280]  Feed給你的東西
[00:06:07.280 --> 00:06:09.280]  你看看很開心,很暢順
[00:06:09.280 --> 00:06:10.280]  又不用想太多
[00:06:10.280 --> 00:06:13.280]  譬如Felix提他老婆的例子也是
[00:06:13.280 --> 00:06:16.280]  因為很舒服,很適合看
[00:06:16.280 --> 00:06:18.280]  那你就會做
[00:06:18.280 --> 00:06:21.280]  其實科技特色就是讓你很舒服
[00:06:21.280 --> 00:06:26.280]  在這裡你真的要決定成為一個有美德的人
[00:06:26.280 --> 00:06:28.280]  你要知道其實你是抗衡一些
[00:06:28.280 --> 00:06:31.280]  你未必很暢順很舒服的東西
[00:06:31.280 --> 00:06:33.280]  所以剛才說的時候
[00:06:33.280 --> 00:06:35.280]  我也會想起屬靈操練
[00:06:35.280 --> 00:06:37.280]  以前我們屬靈操練說的
[00:06:37.280 --> 00:06:39.280]  譬如我們看
[00:06:39.280 --> 00:06:40.280]  我來的年代
[00:06:40.280 --> 00:06:41.280]  看Richard Forster
[00:06:41.280 --> 00:06:44.280]  說怎樣去靜
[00:06:44.280 --> 00:06:46.280]  怎樣去不想事
[00:06:46.280 --> 00:06:49.280]  怎樣去不被挑釁影響自己
[00:06:49.280 --> 00:06:51.280]  而其實同一個操練
[00:06:51.280 --> 00:06:54.280]  是可以在這個情況下
[00:06:54.280 --> 00:06:56.280]  令自己可以有控制
[00:06:56.280 --> 00:07:01.280]  現在其實是公開問問題的時間
[00:07:01.280 --> 00:07:07.280]  我會繼續選一些已經在WhatsApp群組提出的問題
[00:07:07.280 --> 00:07:09.280]  除了這個之外
[00:07:09.280 --> 00:07:11.280]  中間會有咪高峰
[00:07:11.280 --> 00:07:15.280]  如果你想直接問的話
[00:07:15.280 --> 00:07:17.280]  更加生動的話
[00:07:17.280 --> 00:07:20.280]  請你走到咪高峰前面
[00:07:20.280 --> 00:07:21.280]  先不要說話
[00:07:21.280 --> 00:07:23.280]  我知道原來你想問
[00:07:23.280 --> 00:07:25.280]  可能等回答完
[00:07:25.280 --> 00:07:27.280]  我看到就會請你發言
[00:07:27.280 --> 00:07:30.280]  所以如果你想走出來問
[00:07:30.280 --> 00:07:32.280]  現在可以走出來
[00:07:32.280 --> 00:07:34.280]  否則也可以繼續WhatsApp問
[00:07:34.280 --> 00:07:38.280]  不過首先問院長
[00:07:38.280 --> 00:07:42.280]  其實也延續剛才的操練問題
[00:07:42.280 --> 00:07:46.280]  你提到我們要做科技的主人
[00:07:46.280 --> 00:07:49.280]  不要科技做了我們的主人
[00:07:49.280 --> 00:07:51.280]  怎樣才叫科技做了我們的主人呢
[00:07:51.280 --> 00:07:52.280]  舉個例子
[00:07:52.280 --> 00:07:54.280]  最近中晨
[00:07:54.280 --> 00:07:57.280]  我的辦公室的冷氣機壞了
[00:07:57.280 --> 00:08:00.280]  所以我在家工作
[00:08:00.280 --> 00:08:04.280]  幫拓展部說中晨很多設施都很舊了
[00:08:04.280 --> 00:08:06.280]  所以你記不記得
[00:08:06.280 --> 00:08:10.280]  中晨我的辦公室的冷氣機壞了
[00:08:10.280 --> 00:08:12.280]  所以我就留在家工作
[00:08:12.280 --> 00:08:14.280]  因為家裡的冷氣機沒有壞
[00:08:14.280 --> 00:08:17.280]  我是否依靠冷氣呢
[00:08:17.280 --> 00:08:21.280]  我是否用科技讓我可以活得更加好
[00:08:21.280 --> 00:08:26.280]  我已經好像成為了科技的奴隸
[00:08:26.280 --> 00:08:28.280]  沒有它我就死定了
[00:08:28.280 --> 00:08:33.280]  可能呼籲一下奉獻給你房間的冷氣機
[00:08:33.280 --> 00:08:40.280]  其實很難就這樣說什麼是什麼不是
[00:08:40.280 --> 00:08:42.280]  尤其是操練那裡
[00:08:42.280 --> 00:08:44.280]  其實如果你的領袖是不開冷氣
[00:08:44.280 --> 00:08:46.280]  我們知道現在有位香港的名人
[00:08:46.280 --> 00:08:48.280]  付電費那樣
[00:08:48.280 --> 00:08:51.280]  其實我很欣賞
[00:08:51.280 --> 00:08:53.280]  其實很有趣
[00:08:53.280 --> 00:08:54.280]  社會笑他
[00:08:54.280 --> 00:08:57.280]  他動機在各方面
[00:08:57.280 --> 00:08:59.280]  但問題就是這樣
[00:08:59.280 --> 00:09:05.280]  困難的地方就是科技真的不是本身邪惡
[00:09:05.280 --> 00:09:08.280]  但是因為我們用了它
[00:09:08.280 --> 00:09:11.280]  我們就慢慢靠它
[00:09:11.280 --> 00:09:14.280]  你說你不靠它就不行
[00:09:14.280 --> 00:09:19.280]  因為就算創世的第二章耕種也是科技
[00:09:19.280 --> 00:09:24.280]  難道以前的人打獵也是
[00:09:24.280 --> 00:09:26.280]  不過你去到什麼程度而已
[00:09:26.280 --> 00:09:28.280]  所以這些事情
[00:09:28.280 --> 00:09:31.280]  我想有很多因素在背後決定
[00:09:31.280 --> 00:09:33.280]  你個人的領袖
[00:09:33.280 --> 00:09:35.280]  你看看大衛的環境
[00:09:35.280 --> 00:09:40.280]  說真的,你也知道鍾成日老師到今天仍然在開槍
[00:09:40.280 --> 00:09:44.280]  真的有這樣的人
[00:09:44.280 --> 00:09:48.280]  我想就是一種你怎樣操練
[00:09:48.280 --> 00:09:50.280]  還有有趣的地方
[00:09:50.280 --> 00:09:52.280]  人身體有反應
[00:09:52.280 --> 00:09:54.280]  你不開冷氣太多
[00:09:54.280 --> 00:09:56.280]  其實你會習慣
[00:09:56.280 --> 00:09:58.280]  其實是慢慢習慣
[00:09:58.280 --> 00:10:00.280]  因為有在我們不習慣
[00:10:00.280 --> 00:10:02.280]  慢慢我們就覺得不行
[00:10:02.280 --> 00:10:05.280]  這是科技帶領我們走的路
[00:10:05.280 --> 00:10:07.280]  一定要
[00:10:07.280 --> 00:10:09.280]  是不是一定要呢
[00:10:09.280 --> 00:10:12.280]  我經常說創世第一章
[00:10:12.280 --> 00:10:14.280]  甚好的世界而是沒有科技
[00:10:14.280 --> 00:10:16.280]  為什麼今天要呢
[00:10:16.280 --> 00:10:20.280]  這個就是我們經常問的問題
[00:10:20.280 --> 00:10:24.280]  下次行山不要帶蚊子爬水
[00:10:24.280 --> 00:10:27.280]  那就打了針
[00:10:27.280 --> 00:10:28.280]  小心
[00:10:28.280 --> 00:10:30.280]  我想說一點
[00:10:30.280 --> 00:10:32.280]  其實你想一下我們現在坐在這裡
[00:10:32.280 --> 00:10:34.280]  我們的咪高峰
[00:10:34.280 --> 00:10:35.280]  桌子
[00:10:35.280 --> 00:10:36.280]  椅子
[00:10:36.280 --> 00:10:38.280]  其實所有這些東西都是科技
[00:10:38.280 --> 00:10:41.280]  我們就在這個科技的環境生活
[00:10:41.280 --> 00:10:46.280]  問題就是什麼時候科技會令我們沒有空間
[00:10:46.280 --> 00:10:50.280]  去做我們想做的事
[00:10:50.280 --> 00:10:52.280]  如果沒有了就是問題
[00:10:52.280 --> 00:10:55.280]  這是科技整個發展的問題
[00:10:55.280 --> 00:11:01.280]  有時候普通人未必有那個位置去改變
[00:11:01.280 --> 00:11:05.280]  但如果我們希望它繼續發展出來
[00:11:05.280 --> 00:11:10.280]  就有空間讓大家繼續發揮做人的本性
[00:11:10.280 --> 00:11:14.280]  有一個問題
[00:11:14.280 --> 00:11:16.280]  今天有問
[00:11:16.280 --> 00:11:19.280]  我想不少基督徒在網上都問過
[00:11:19.280 --> 00:11:24.280]  現在check gbd可以整篇文章寫出來
[00:11:24.280 --> 00:11:26.280]  可不可以問check gbd
[00:11:26.280 --> 00:11:30.280]  根據馬太福音三宗十七節應該怎樣講道
[00:11:30.280 --> 00:11:33.280]  如果check gbd弄了篇講道出來
[00:11:33.280 --> 00:11:36.280]  到底有沒有聖靈帶領的呢
[00:11:36.280 --> 00:11:37.280]  AI
[00:11:37.280 --> 00:11:40.280]  我可不可以先回答
[00:11:40.280 --> 00:11:42.280]  我試過聽完篇講道
[00:11:42.280 --> 00:11:45.280]  然後我問check gbd
[00:11:45.280 --> 00:11:47.280]  叫他講同一個話題
[00:11:47.280 --> 00:11:48.280]  其實是很廢的
[00:11:48.280 --> 00:11:50.280]  所以暫時來說
[00:11:50.280 --> 00:11:53.280]  牧師的工作是安全的
[00:11:59.280 --> 00:12:03.280]  我想大家可能都會八卦
[00:12:03.280 --> 00:12:06.280]  或者用實驗性質試過打一些東西
[00:12:06.280 --> 00:12:10.280]  然後問他做哪裏等等
[00:12:10.280 --> 00:12:13.280]  我想一開始你會覺得很神奇
[00:12:13.280 --> 00:12:14.280]  為甚麼可以做到
[00:12:14.280 --> 00:12:17.280]  但當你細心讀他的內容的時候
[00:12:17.280 --> 00:12:22.280]  你會發現對我的生命有甚麼幫助
[00:12:22.280 --> 00:12:27.280]  真的很普通
[00:12:27.280 --> 00:12:30.280]  講道也好,或者是悉經也好
[00:12:30.280 --> 00:12:34.280]  其實很多時候是講講者的生命
[00:12:34.280 --> 00:12:36.280]  他自己的經歷
[00:12:36.280 --> 00:12:38.280]  當他跟你講的時候
[00:12:38.280 --> 00:12:41.280]  那個訊息是他的生命與你的對話
[00:12:41.280 --> 00:12:44.280]  那個才是對我們最大的影響
[00:12:44.280 --> 00:12:45.280]  對我來說是
[00:12:45.280 --> 00:12:48.280]  如果有一天有人去舉辦
[00:12:48.280 --> 00:12:50.280]  香港舉辦一個AI的崇拜
[00:12:50.280 --> 00:12:52.280]  我會參加
[00:12:52.280 --> 00:12:54.280]  我會看看你在做甚麼
[00:12:54.280 --> 00:12:59.280]  我不會覺得在當中會有甚麼生命被祝福
[00:12:59.280 --> 00:13:02.280]  其實問題有很多層次
[00:13:02.280 --> 00:13:08.280]  如果用AI來幫你做一些基本的資料搜集
[00:13:08.280 --> 00:13:09.280]  我覺得是很…
[00:13:09.280 --> 00:13:11.280]  你要知道它是否準確
[00:13:11.280 --> 00:13:13.280]  我曾經問過AI
[00:13:13.280 --> 00:13:15.280]  問他一些關於創世紀的人物
[00:13:15.280 --> 00:13:17.280]  他給了我摩西
[00:13:17.280 --> 00:13:20.280]  無論如何,很低級的錯誤都會發生
[00:13:20.280 --> 00:13:21.280]  當然隨著年日
[00:13:21.280 --> 00:13:23.280]  他應該會有些進步
[00:13:23.280 --> 00:13:24.280]  我的意思是
[00:13:24.280 --> 00:13:26.280]  即使你今天問他也要很小心
[00:13:26.280 --> 00:13:28.280]  不要完全相信他
[00:13:28.280 --> 00:13:31.280]  你說預備港島
[00:13:31.280 --> 00:13:33.280]  其實有基本的資料告訴你
[00:13:33.280 --> 00:13:35.280]  某些聖經背景
[00:13:35.280 --> 00:13:36.280]  你查過方便
[00:13:36.280 --> 00:13:37.280]  其實是可以的
[00:13:37.280 --> 00:13:39.280]  你用悉經書也是這樣
[00:13:39.280 --> 00:13:41.280]  不過預備訊息
[00:13:41.280 --> 00:13:42.280]  我們都知道
[00:13:42.280 --> 00:13:46.280]  預備訊息其實我們有很多悉經書的工具
[00:13:46.280 --> 00:13:49.280]  不過悉經書是幫不到你真正的訊息出現
[00:13:49.280 --> 00:13:51.280]  如果有同工或聽港島的
[00:13:51.280 --> 00:13:52.280]  你才會知道
[00:13:52.280 --> 00:13:54.280]  那是一種生命的東西
[00:13:54.280 --> 00:13:56.280]  預備就沒有問題
[00:13:56.280 --> 00:13:59.280]  你說是否有一個AI的機械人在港島
[00:13:59.280 --> 00:14:00.280]  是不可以的
[00:14:00.280 --> 00:14:02.280]  有一個例子就是
[00:14:02.280 --> 00:14:04.280]  保羅在提瑪撒前說
[00:14:04.280 --> 00:14:05.280]  「我是一個罪魁」
[00:14:05.280 --> 00:14:07.280]  機械人怎麼做罪魁呢?
[00:14:07.280 --> 00:14:09.280]  那是一個生命
[00:14:09.280 --> 00:14:12.280]  那是上帝跟傳道人所說的話
[00:14:12.280 --> 00:14:16.280]  讓他生命跟上帝去接觸
[00:14:16.280 --> 00:14:17.280]  聆聽
[00:14:17.280 --> 00:14:20.280]  然後透過他生命再說給那個會眾
[00:14:20.280 --> 00:14:22.280]  這是傳道人的工作
[00:14:22.280 --> 00:14:26.280]  即使那個技巧或文章開設得好
[00:14:26.280 --> 00:14:29.280]  這是目者的訊息說到
[00:14:29.280 --> 00:14:31.280]  其實我的看法是不能被代替
[00:14:31.280 --> 00:14:41.280]  如果你看回GPT或Deep Learning做到的東西
[00:14:41.280 --> 00:14:43.280]  就是Recognize Patterns
[00:14:43.280 --> 00:14:46.280]  所以他可以將很多不同的講章
[00:14:46.280 --> 00:14:47.280]  Recognize Patterns
[00:14:47.280 --> 00:14:48.280]  砌一些東西出來
[00:14:48.280 --> 00:14:50.280]  但是他做不到的東西
[00:14:50.280 --> 00:14:52.280]  就是一些任何新的事物
[00:14:52.280 --> 00:14:54.280]  發生過的東西
[00:14:54.280 --> 00:14:56.280]  他是不能夠對應的
[00:14:56.280 --> 00:15:00.280]  所以一個講章如果是好的話
[00:15:00.280 --> 00:15:04.280]  應該是對應會眾的生活要面對的問題
[00:15:04.280 --> 00:15:05.280]  如果這樣的話
[00:15:05.280 --> 00:15:08.280]  其實GPT應該永遠都不能做到
[00:15:08.280 --> 00:15:11.280]  OK
[00:15:11.280 --> 00:15:13.280]  好,沒有人出來
[00:15:13.280 --> 00:15:17.280]  繼續問一些網上問的問題
[00:15:17.280 --> 00:15:20.280]  其實剛才提過一點
[00:15:20.280 --> 00:15:21.280]  好像是Felix
[00:15:21.280 --> 00:15:22.280]  不緊要是誰都好
[00:15:22.280 --> 00:15:25.280]  就是那個系統
[00:15:25.280 --> 00:15:26.280]  你提過系統
[00:15:26.280 --> 00:15:27.280]  不過好像Alex也提過
[00:15:27.280 --> 00:15:30.280]  就是技術都會形成一個人
[00:15:30.280 --> 00:15:31.280]  其實院長也有提過
[00:15:31.280 --> 00:15:32.280]  那個手取的問題
[00:15:32.280 --> 00:15:35.280]  即是手起枕那個
[00:15:35.280 --> 00:15:38.280]  其實你想像當譬如
[00:15:38.280 --> 00:15:40.280]  Check GPD或是越來越多AI
[00:15:40.280 --> 00:15:42.280]  或者好就是那些數學
[00:15:42.280 --> 00:15:44.280]  或者Check GPD不知道是好還是不好
[00:15:44.280 --> 00:15:46.280]  或者甚至比較malificious
[00:15:46.280 --> 00:15:48.280]  譬如好像騙你
[00:15:48.280 --> 00:15:50.280]  用AI走來騙你
[00:15:50.280 --> 00:15:54.280]  即是AI在我們的生活越來越多的時候
[00:15:54.280 --> 00:15:58.280]  其實對人的mentality會怎樣影響呢
[00:15:58.280 --> 00:16:01.280]  即是不是那個很big的story
[00:16:01.280 --> 00:16:03.280]  是比較concrete一點
[00:16:03.280 --> 00:16:05.280]  即是我經常對著手機
[00:16:05.280 --> 00:16:06.280]  或者經常和手機聊天
[00:16:06.280 --> 00:16:07.280]  和Check GPD聊天
[00:16:07.280 --> 00:16:10.280]  其實對人的mentality
[00:16:10.280 --> 00:16:13.280]  或者我們的心靈其實會有什麼影響呢
[00:16:13.280 --> 00:16:15.280]  我搶答先吧
[00:16:15.280 --> 00:16:17.280]  我不知道大家有沒有
[00:16:17.280 --> 00:16:20.280]  這個也是自我反省的一個過程
[00:16:20.280 --> 00:16:26.280]  近幾年或者自從手機用得多之後
[00:16:26.280 --> 00:16:29.280]  不知道大家對於耐性這件事
[00:16:29.280 --> 00:16:31.280]  是不是會弱了
[00:16:31.280 --> 00:16:35.280]  即是很多時候科技是告訴我們
[00:16:35.280 --> 00:16:38.280]  我要的東西我要的答案
[00:16:38.280 --> 00:16:39.280]  是要很即時
[00:16:39.280 --> 00:16:42.280]  即是以前我們想小時候的時候
[00:16:42.280 --> 00:16:43.280]  未必是
[00:16:43.280 --> 00:16:46.280]  即是現在我甚至
[00:16:46.280 --> 00:16:48.280]  剛才說開教育
[00:16:48.280 --> 00:16:49.280]  計算機
[00:16:49.280 --> 00:16:53.280]  我們以前真的要找人手計算
[00:16:53.280 --> 00:16:54.280]  現在也要的
[00:16:54.280 --> 00:16:56.280]  某些小學生也是人手計算
[00:16:56.280 --> 00:16:57.280]  去計算的時候
[00:16:57.280 --> 00:17:01.280]  其實是一個操練耐性的過程
[00:17:01.280 --> 00:17:02.280]  即是那種技巧
[00:17:02.280 --> 00:17:07.280]  但其實是如果說mentality的改變
[00:17:07.280 --> 00:17:09.280]  其實我不知道大家對於
[00:17:09.280 --> 00:17:13.280]  人與人之間的耐性
[00:17:13.280 --> 00:17:15.280]  即是我發了一個message
[00:17:15.280 --> 00:17:17.280]  是要秒回的
[00:17:17.280 --> 00:17:20.280]  年輕人的術語叫做秒回
[00:17:20.280 --> 00:17:22.280]  如果不是秒回
[00:17:22.280 --> 00:17:26.280]  是不是在著緊我
[00:17:26.280 --> 00:17:27.280]  或者發生什麼事
[00:17:27.280 --> 00:17:32.280]  這些是影響了我們與人之間的交流
[00:17:32.280 --> 00:17:34.280]  我們的耐性是弱了
[00:17:34.280 --> 00:17:35.280]  如果我說第一件事
[00:17:35.280 --> 00:17:41.280]  就是對於那種耐性失去的東西是弱了
[00:17:41.280 --> 00:17:43.280]  我就繼續回答
[00:17:43.280 --> 00:17:47.280]  其實當我們與教育委員會談話
[00:17:47.280 --> 00:17:48.280]  他很禮貌的
[00:17:48.280 --> 00:17:50.280]  你怎樣說他都會
[00:17:50.280 --> 00:17:51.280]  他做錯事的時候
[00:17:51.280 --> 00:17:53.280]  他會說I apologize第一句
[00:17:53.280 --> 00:17:54.280]  很舒服
[00:17:54.280 --> 00:17:56.280]  無論你怎樣罵他
[00:17:56.280 --> 00:17:57.280]  他都是這樣的
[00:17:57.280 --> 00:18:00.280]  很舒服的
[00:18:00.280 --> 00:18:01.280]  很吸引的
[00:18:01.280 --> 00:18:04.280]  你明白了
[00:18:04.280 --> 00:18:07.280]  其實我們對人不是這樣的
[00:18:07.280 --> 00:18:11.280]  人有一種叫做自由
[00:18:11.280 --> 00:18:14.280]  自由簡單來說就是你猜不到他
[00:18:14.280 --> 00:18:17.280]  猜不到他往往他的回應
[00:18:17.280 --> 00:18:20.280]  會令你不開心和不舒服
[00:18:20.280 --> 00:18:22.280]  因為人是這樣的
[00:18:22.280 --> 00:18:27.280]  其實科技是能夠讓我們舒舒服服
[00:18:27.280 --> 00:18:30.280]  所以就算check gdp的回應
[00:18:30.280 --> 00:18:32.280]  除非你寫一個program出來
[00:18:32.280 --> 00:18:34.280]  就沒有人喜歡
[00:18:34.280 --> 00:18:36.280]  因為科技是令你舒服
[00:18:36.280 --> 00:18:44.280]  它不會產生一些令你不舒服的東西
[00:18:44.280 --> 00:18:46.280]  但其實人與人的交往
[00:18:46.280 --> 00:18:48.280]  人與人的交往是這樣的
[00:18:48.280 --> 00:18:53.280]  甚至有些科技的哲學家幻想
[00:18:53.280 --> 00:18:55.280]  可能到某一個程度
[00:18:55.280 --> 00:18:58.280]  譬如有virtual reality
[00:18:58.280 --> 00:19:01.280]  一個人可能已經發展到一個地步
[00:19:01.280 --> 00:19:06.280]  我由頭到尾不需要和一些令我不開心的人接觸
[00:19:06.280 --> 00:19:11.280]  所有人都只是被一些很舒服的科技AI圍著
[00:19:11.280 --> 00:19:13.280]  問的問題好不好呢?
[00:19:13.280 --> 00:19:15.280]  可能人會覺得很好
[00:19:15.280 --> 00:19:18.280]  我們值得問,是否真的想這樣的世界?
[00:19:18.280 --> 00:19:19.280]  甚麼叫做人?
[00:19:19.280 --> 00:19:22.280]  我們剛才說了操練美德
[00:19:22.280 --> 00:19:24.280]  操練美德何時發生?
[00:19:24.280 --> 00:19:26.280]  是當你見到一些很難受的操練到
[00:19:26.280 --> 00:19:28.280]  你對著一些很舒服的人
[00:19:28.280 --> 00:19:29.280]  你沒有美德可操練
[00:19:29.280 --> 00:19:30.280]  甚麼叫彼此相愛?
[00:19:30.280 --> 00:19:32.280]  因為那些不可愛的人在你面前
[00:19:32.280 --> 00:19:34.280]  你可愛的人每個人都喜歡
[00:19:34.280 --> 00:19:36.280]  所以很有趣
[00:19:36.280 --> 00:19:38.280]  科技究竟是如何塑造我們呢?
[00:19:38.280 --> 00:19:44.280]  一些這麼方便、開心的東西究竟是好還是不好呢?
[00:19:44.280 --> 00:19:47.280]  就問我們做人的目標是甚麼
[00:19:47.280 --> 00:19:51.280]  其實是一個終末觀
[00:19:51.280 --> 00:19:55.280]  即是我的目標,目的想做一個甚麼人
[00:19:55.280 --> 00:19:57.280]  其實是要回答這個問題
[00:19:58.280 --> 00:20:04.280]  我想說一個問題令我想起
[00:20:04.280 --> 00:20:09.280]  我在想高科技和科技是有分別的
[00:20:09.280 --> 00:20:13.280]  高科技變得快到你適應不到
[00:20:13.280 --> 00:20:18.280]  如果你想像我們所有科技都攤長二億年慢慢轉變
[00:20:18.280 --> 00:20:21.280]  那些人可能容易很多去適應
[00:20:21.280 --> 00:20:24.280]  但因為在你未適應到一個新的科技之前
[00:20:24.280 --> 00:20:28.280]  譬如說社交媒體突然間又說AI
[00:20:28.280 --> 00:20:30.280]  或者一百年前都沒有飛機
[00:20:30.280 --> 00:20:31.280]  對於我祖母來說
[00:20:31.280 --> 00:20:35.280]  你給她電腦她沒可能知道發生甚麼事
[00:20:35.280 --> 00:20:37.280]  可能到了五十年之後
[00:20:37.280 --> 00:20:39.280]  我死了五十年之後
[00:20:39.280 --> 00:20:41.280]  三十年之後到我老了的時候
[00:20:41.280 --> 00:20:44.280]  可能已經有很多東西我適應不到
[00:20:44.280 --> 00:20:47.280]  而我想其中一個問題要面對
[00:20:47.280 --> 00:20:49.280]  為甚麼這麼大問題呢?
[00:20:49.280 --> 00:20:51.280]  就是因為那些東西來得太快
[00:20:51.280 --> 00:20:55.280]  整個社會都沒有空間去適應
[00:20:55.280 --> 00:20:57.280]  引起很多問題
[00:20:57.280 --> 00:20:59.280]  其實也有些問題
[00:20:59.280 --> 00:21:04.280]  Alex剛才很強調監管
[00:21:04.280 --> 00:21:07.280]  問題是現在的科技走得這麼快
[00:21:07.280 --> 00:21:11.280]  固然像我這些凡夫俗子都未搞定
[00:21:11.280 --> 00:21:13.280]  怎樣去監管呢?
[00:21:13.280 --> 00:21:15.280]  有人說Twitter
[00:21:15.280 --> 00:21:17.280]  現在好像Twitter改了名叫X
[00:21:17.280 --> 00:21:19.280]  以前經常說Twitter
[00:21:19.280 --> 00:21:21.280]  傳來傳去
[00:21:21.280 --> 00:21:22.280]  我都不搞了
[00:21:22.280 --> 00:21:24.280]  我只是搞Facebook一個已經頭都大了
[00:21:24.280 --> 00:21:28.280]  所以其他Twitter、Line那些我不管了
[00:21:28.280 --> 00:21:32.280]  你不懂你怎樣去監管呢?
[00:21:32.280 --> 00:21:34.280]  這個是比較個人的水平
[00:21:34.280 --> 00:21:36.280]  就算是社會水平來說
[00:21:36.280 --> 00:21:37.280]  監管
[00:21:37.280 --> 00:21:42.280]  其實我覺得奧巴馬電影是頗啟迪的
[00:21:42.280 --> 00:21:44.280]  奧巴馬發展的原子彈
[00:21:44.280 --> 00:21:47.280]  最後是否扔掉是Truman決定的
[00:21:47.280 --> 00:21:50.280]  所以很多時候最後所謂的監管權
[00:21:50.280 --> 00:21:53.280]  其實在社會裏面是depend on
[00:21:53.280 --> 00:21:55.280]  那些politician
[00:21:55.280 --> 00:21:58.280]  和depend on那些大商家
[00:21:58.280 --> 00:22:00.280]  Aaron Marx那些人
[00:22:00.280 --> 00:22:01.280]  經常說那些
[00:22:01.280 --> 00:22:03.280]  我覺得Aaron Marx經常說那些瘋狂的東西
[00:22:03.280 --> 00:22:05.280]  你想想
[00:22:05.280 --> 00:22:07.280]  如果你經常說監管
[00:22:07.280 --> 00:22:09.280]  我們要將監管權交在
[00:22:09.280 --> 00:22:13.280]  這批政客和這批大商家當中
[00:22:13.280 --> 00:22:15.280]  到底是不是realistic呢?
[00:22:15.280 --> 00:22:17.280]  到底怎樣監管呢?
[00:22:17.280 --> 00:22:21.280]  我想補充一件事
[00:22:21.280 --> 00:22:23.280]  因為我那張slide就有說
[00:22:23.280 --> 00:22:25.280]  Elon Musk有petition
[00:22:25.280 --> 00:22:27.280]  去暫停那個GDP的development
[00:22:27.280 --> 00:22:29.280]  而大部分人認為
[00:22:29.280 --> 00:22:31.280]  因為他想自己develop自己那套東西
[00:22:31.280 --> 00:22:35.280]  所以就要求他暫停
[00:22:35.280 --> 00:22:39.280]  我想我剛才是很強調
[00:22:39.280 --> 00:22:41.280]  正正是因為我們沒有辦法
[00:22:41.280 --> 00:22:45.280]  去在權力的角力裏面
[00:22:45.280 --> 00:22:47.280]  去有那種話事權
[00:22:47.280 --> 00:22:52.280]  所以反而我們能夠做的事
[00:22:52.280 --> 00:22:54.280]  首先是我們自己
[00:22:54.280 --> 00:22:56.280]  作為基督徒教會
[00:22:56.280 --> 00:22:58.280]  就是怎樣去demonstrate一種
[00:22:58.280 --> 00:23:02.280]  有德性、有道德的使用者
[00:23:02.280 --> 00:23:04.280]  或者是一個開發者
[00:23:04.280 --> 00:23:05.280]  是甚麼來的呢?
[00:23:05.280 --> 00:23:08.280]  讓這個世界其實可以見到
[00:23:08.280 --> 00:23:10.280]  原來是有一種不同的可能性
[00:23:10.280 --> 00:23:13.280]  然後這些影響力會不會慢慢地
[00:23:13.280 --> 00:23:18.280]  滲透到我們一些政客的手上呢?
[00:23:18.280 --> 00:23:19.280]  我不知道
[00:23:19.280 --> 00:23:21.280]  不過我想上帝給我們的
[00:23:21.280 --> 00:23:25.280]  就是有能力在被影響的空間
[00:23:25.280 --> 00:23:26.280]  首先我們要做好自己
[00:23:26.280 --> 00:23:29.280]  這是我自己的看法
[00:23:29.280 --> 00:23:32.280]  我想說兩個難題
[00:23:32.280 --> 00:23:34.280]  對於剛才說監管的東西
[00:23:34.280 --> 00:23:35.280]  第一個就是
[00:23:35.280 --> 00:23:37.280]  就好像那齣戲裡面
[00:23:37.280 --> 00:23:39.280]  Oppenheimer的戲裡面
[00:23:39.280 --> 00:23:41.280]  因為不同的國家
[00:23:41.280 --> 00:23:43.280]  或者不同的公司都在競爭
[00:23:43.280 --> 00:23:44.280]  有些人就會說
[00:23:44.280 --> 00:23:47.280]  如果我們這個國家說暫停的
[00:23:47.280 --> 00:23:48.280]  別人說是發展的
[00:23:48.280 --> 00:23:50.280]  那我們就全輸了
[00:23:50.280 --> 00:23:53.280]  現在說的不是剛才說的GPT那些東西
[00:23:53.280 --> 00:23:55.280]  很多軍事的武器
[00:23:55.280 --> 00:23:57.280]  其實都是用AI來做的
[00:23:57.280 --> 00:23:59.280]  有個朋友就跟我說
[00:23:59.280 --> 00:24:01.280]  Ukraine用的武器
[00:24:01.280 --> 00:24:05.280]  其實很多都是用Drones或者AI來做的
[00:24:05.280 --> 00:24:07.280]  這是其中一個問題
[00:24:07.280 --> 00:24:08.280]  第二個問題就是
[00:24:08.280 --> 00:24:11.280]  現在用的那套Deep Learning的做法
[00:24:11.280 --> 00:24:13.280]  其實是沒有人可以知道裡面發生什麼事的
[00:24:13.280 --> 00:24:17.280]  就是by default一定是沒有可能知道裡面發生什麼事的
[00:24:17.280 --> 00:24:21.280]  所以如果要解釋他為什麼達到那個決定
[00:24:21.280 --> 00:24:22.280]  其實是
[00:24:22.280 --> 00:24:24.280]  你不如叫他不要做算了
[00:24:24.280 --> 00:24:30.280]  譬如你說為什麼GPT可以分析到我們說話的呢?
[00:24:30.280 --> 00:24:32.280]  其實沒有人真的知道
[00:24:32.280 --> 00:24:34.280]  你最多可以做到猜猜的
[00:24:34.280 --> 00:24:35.280]  但是做不到
[00:24:35.280 --> 00:24:36.280]  如果做不到的時候
[00:24:36.280 --> 00:24:40.280]  剛才說的那種transparency的要求
[00:24:40.280 --> 00:24:42.280]  其實是很難做到的
[00:24:42.280 --> 00:24:48.280]  這個對於現在做研究的人來說是一個謎
[00:24:48.280 --> 00:24:50.280]  補充一點
[00:24:50.280 --> 00:24:53.280]  我以前也聽過所謂的Deep Learning
[00:24:53.280 --> 00:24:54.280]  其實很簡單的說法就是
[00:24:54.280 --> 00:24:56.280]  你叫電腦自己和自己下棋
[00:24:56.280 --> 00:24:59.280]  電腦下了幾天
[00:24:59.280 --> 00:25:01.280]  可能下了一萬盤棋
[00:25:01.280 --> 00:25:02.280]  他就學會下棋了
[00:25:02.280 --> 00:25:05.280]  他就比全世界下棋最厲害的人厲害了
[00:25:05.280 --> 00:25:07.280]  但是你不知道電腦怎麼想的
[00:25:07.280 --> 00:25:09.280]  他只是自己和自己下了一輪
[00:25:09.280 --> 00:25:11.280]  他就學會下棋了
[00:25:11.280 --> 00:25:13.280]  這個就是Deep Learning
[00:25:13.280 --> 00:25:16.280]  所以我們根本不知道裡面發生什麼事
[00:25:16.280 --> 00:25:19.280]  所以聽完你說也挺悲觀的
[00:25:19.280 --> 00:25:22.280]  有些研究是用解釋AI的
[00:25:22.280 --> 00:25:25.280]  嘗試說我想讓他真的解釋得到
[00:25:25.280 --> 00:25:27.280]  只不過我自己覺得他做不到
[00:25:27.280 --> 00:25:29.280]  可能有些人真的做到
[00:25:29.280 --> 00:25:31.280]  但是我覺得他做到的可能性就是
[00:25:31.280 --> 00:25:34.280]  在中間加多幾個步驟
[00:25:34.280 --> 00:25:36.280]  而步驟是可以解釋的
[00:25:36.280 --> 00:25:38.280]  不過步驟裡面的東西仍然解釋不了
[00:25:38.280 --> 00:25:42.280]  有點頭痛
[00:25:42.280 --> 00:25:43.280]  Anyway
[00:25:43.280 --> 00:25:47.280]  有人問了一個挺有趣的問題
[00:25:47.280 --> 00:25:49.280]  他說你剛才說的AI
[00:25:49.280 --> 00:25:55.280]  都是集中對香港、美國這些所謂現代社會的影響
[00:25:55.280 --> 00:25:58.280]  他想問特別對貧窮世界
[00:25:58.280 --> 00:26:02.280]  其實這些AI或者最尖端的科技
[00:26:02.280 --> 00:26:04.280]  會有什麼影響呢?
[00:26:04.280 --> 00:26:06.280]  你猜是好的多壞的多?
[00:26:06.280 --> 00:26:21.280]  其實有一個現象也不只是AI
[00:26:21.280 --> 00:26:23.280]  普遍的科技
[00:26:23.280 --> 00:26:25.280]  有一個叫Digital Divide
[00:26:25.280 --> 00:26:29.280]  有些人就能夠掌控到知道
[00:26:29.280 --> 00:26:32.280]  因為現在的世界
[00:26:32.280 --> 00:26:37.280]  這些科技、AI當然是用來更加有能力去掌控世界
[00:26:37.280 --> 00:26:41.280]  所以一定是一種權力上的差異
[00:26:41.280 --> 00:26:48.280]  永遠都是發展了的那些所謂著數
[00:26:48.280 --> 00:26:50.280]  然後就去敲或者挪奕
[00:26:50.280 --> 00:26:56.280]  那個會是一個不好、一個不公平的現象
[00:26:56.280 --> 00:27:01.280]  我想那個看法都離不開
[00:27:01.280 --> 00:27:07.280]  如何能夠在制度上或做法上幫助到
[00:27:07.280 --> 00:27:14.280]  令到更多人能夠接觸到、運用到、有益到
[00:27:14.280 --> 00:27:16.280]  這是一個很初步的答案
[00:27:16.280 --> 00:27:18.280]  你會否有不同的看法呢?
[00:27:18.280 --> 00:27:19.280]  你說起Digital Divide
[00:27:19.280 --> 00:27:23.280]  我就有一個比較吊詭的回應
[00:27:23.280 --> 00:27:26.280]  正正因為他們不在網上
[00:27:26.280 --> 00:27:28.280]  就不會收集到他們的資料
[00:27:28.280 --> 00:27:29.280]  他們反而會好一點
[00:27:29.280 --> 00:27:31.280]  起碼不會被人操縱
[00:27:31.280 --> 00:27:34.280]  他們仍然可以很開心地在大自然生活
[00:27:34.280 --> 00:27:37.280]  我們就被人操控自己都不知道
[00:27:37.280 --> 00:27:41.280]  所以問題是甚麼叫做好一點呢?
[00:27:41.280 --> 00:27:43.280]  有趣的地方就是
[00:27:43.280 --> 00:27:46.280]  剛才你說不應該看那些feed
[00:27:46.280 --> 00:27:47.280]  給你feed就去
[00:27:47.280 --> 00:27:50.280]  然後你的購物的pattern又被人知道了
[00:27:50.280 --> 00:27:51.280]  可能反過來
[00:27:51.280 --> 00:27:53.280]  他正正就是給我要的東西
[00:27:53.280 --> 00:27:55.280]  我很喜歡被人操控
[00:27:55.280 --> 00:27:56.280]  很有趣的
[00:27:56.280 --> 00:27:58.280]  究竟甚麼叫做好一點呢?
[00:27:58.280 --> 00:28:04.280]  可能有人會覺得被操控、被監控是很好的
[00:28:04.280 --> 00:28:07.280]  所以《美麗新世界》的故事就是
[00:28:07.280 --> 00:28:11.280]  一班很喜歡生活的人
[00:28:11.280 --> 00:28:14.280]  我又落地多一點
[00:28:14.280 --> 00:28:17.280]  這個問題我又想起
[00:28:17.280 --> 00:28:19.280]  幾年前有個畫面
[00:28:19.280 --> 00:28:21.280]  大家記得疫情的時候
[00:28:21.280 --> 00:28:25.280]  有些地方要拿著手機才能進去
[00:28:25.280 --> 00:28:27.280]  甚至超級市場、街市
[00:28:27.280 --> 00:28:28.280]  大家有印象
[00:28:28.280 --> 00:28:31.280]  我經常很深刻的畫面就是
[00:28:31.280 --> 00:28:34.280]  有些婆婆或者老人家
[00:28:34.280 --> 00:28:36.280]  不用去到非洲那麼遠
[00:28:36.280 --> 00:28:39.280]  你只需要一些在科技上
[00:28:39.280 --> 00:28:42.280]  能力相對比較弱的人
[00:28:42.280 --> 00:28:46.280]  我們在一個科技社會裏
[00:28:46.280 --> 00:28:49.280]  他們就會成為那種很弱勢的人
[00:28:49.280 --> 00:28:51.280]  所以剛才也有提及
[00:28:51.280 --> 00:28:56.280]  我們作為使用者
[00:28:56.280 --> 00:29:00.280]  我們如何確保這班弱勢
[00:29:00.280 --> 00:29:02.280]  需要照顧的人
[00:29:02.280 --> 00:29:05.280]  得到他們生活上應有的尊嚴
[00:29:05.280 --> 00:29:07.280]  這個是要去想的問題
[00:29:07.280 --> 00:29:08.280]  我補充一句
[00:29:08.280 --> 00:29:10.280]  當然剛才Alex說的
[00:29:10.280 --> 00:29:11.280]  不過我們又覺得
[00:29:11.280 --> 00:29:13.280]  因為科技發展到一個地步
[00:29:13.280 --> 00:29:15.280]  我們有很多假設
[00:29:15.280 --> 00:29:18.280]  如果中辰你要來報讀
[00:29:18.280 --> 00:29:21.280]  現在是不可能沒有電腦的
[00:29:21.280 --> 00:29:25.280]  但我們在沒有任何一年
[00:29:25.280 --> 00:29:26.280]  突然在申請表上說
[00:29:26.280 --> 00:29:28.280]  你一定要有電腦才能來讀
[00:29:28.280 --> 00:29:29.280]  是不是?
[00:29:29.280 --> 00:29:30.280]  很有趣的
[00:29:30.280 --> 00:29:32.280]  我們做了很多假設
[00:29:32.280 --> 00:29:33.280]  可能我讀的年代
[00:29:33.280 --> 00:29:35.280]  真的沒有電腦也可以的
[00:29:35.280 --> 00:29:37.280]  可以原稿子交功課
[00:29:37.280 --> 00:29:39.280]  但今天是完全不行的
[00:29:39.280 --> 00:29:42.280]  我們中間沒有做過任何一個動作
[00:29:42.280 --> 00:29:45.280]  是要要求你有電腦
[00:29:45.280 --> 00:29:47.280]  或是懂電腦才能讀書
[00:29:47.280 --> 00:29:49.280]  很有趣的就是
[00:29:49.280 --> 00:29:53.280]  有沒有科技的問題
[00:29:53.280 --> 00:29:55.280]  仿佛我們是有部份
[00:29:55.280 --> 00:29:56.280]  而我們現在有的人
[00:29:56.280 --> 00:29:58.280]  是完全不會想像到
[00:29:58.280 --> 00:30:00.280]  沒有是一個怎樣的世界
[00:30:00.280 --> 00:30:04.280]  所以現在你沒有智能電話的話
[00:30:04.280 --> 00:30:07.280]  你去到餐廳也可能點不到菜
[00:30:07.280 --> 00:30:09.280]  掃描不到QR Code
[00:30:09.280 --> 00:30:15.280]  所以要繼續想這個問題
[00:30:15.280 --> 00:30:20.280]  或者轉一轉方向
[00:30:20.280 --> 00:30:21.280]  再次鼓勵
[00:30:21.280 --> 00:30:25.280]  有人想站出來做勇敢的AI人
[00:30:25.280 --> 00:30:28.280]  站出來問問題是很歡迎的
[00:30:28.280 --> 00:30:31.280]  不過未有人站出來再問一個問題
[00:30:31.280 --> 00:30:32.280]  也是有人問的
[00:30:32.280 --> 00:30:36.280]  方向有一點點有關
[00:30:36.280 --> 00:30:37.280]  變了一點點
[00:30:37.280 --> 00:30:41.280]  作為一個堂會的教授
[00:30:41.280 --> 00:30:45.280]  其實你覺得堂會教授
[00:30:45.280 --> 00:30:51.280]  有什麼責任幫助會友去面對AI世代呢?
[00:30:51.280 --> 00:30:53.280]  無論是年長的
[00:30:53.280 --> 00:30:55.280]  死死也
[00:30:55.280 --> 00:30:58.280]  有沒有責任幫助
[00:30:58.280 --> 00:30:59.280]  或者年輕的
[00:30:59.280 --> 00:31:02.280]  他們將來面對很多AI的東西
[00:31:02.280 --> 00:31:03.280]  會爆出來
[00:31:03.280 --> 00:31:04.280]  想都想不到
[00:31:04.280 --> 00:31:10.280]  堂會所謂牧養或者門徒訓練的時候
[00:31:10.280 --> 00:31:14.280]  有沒有什麼可以做或者應該做的呢?
[00:31:14.280 --> 00:31:18.280]  誰都可以答
[00:31:18.280 --> 00:31:20.280]  我們三個都不是堂會負責人
[00:31:20.280 --> 00:31:24.280]  其實這個問題
[00:31:24.280 --> 00:31:26.280]  我覺得問什麼是教牧
[00:31:26.280 --> 00:31:28.280]  多於問AI
[00:31:28.280 --> 00:31:31.280]  其實你說教牧是在講牧養人
[00:31:31.280 --> 00:31:34.280]  關顧他整個人的生命
[00:31:34.280 --> 00:31:36.280]  而AI是這麼重要的一部分
[00:31:36.280 --> 00:31:38.280]  當然是責無旁貸
[00:31:38.280 --> 00:31:41.280]  不過世界上真的有這麼多東西
[00:31:41.280 --> 00:31:44.280]  其實我想問題是什麼呢?
[00:31:44.280 --> 00:31:46.280]  就是要留意當你的會眾
[00:31:46.280 --> 00:31:51.280]  你給某一樣東西影響得深遠
[00:31:51.280 --> 00:31:53.280]  那個就成為你牧養的議題
[00:31:53.280 --> 00:31:55.280]  我會覺得是這樣
[00:31:55.280 --> 00:31:57.280]  當然當你去到這個位置的時候
[00:31:57.280 --> 00:31:59.280]  你的困難的地方就在於
[00:31:59.280 --> 00:32:02.280]  這個教牧需要知道到什麼深度
[00:32:02.280 --> 00:32:04.280]  究竟影響怎樣
[00:32:04.280 --> 00:32:09.280]  實質上是有些困難的
[00:32:09.280 --> 00:32:11.280]  我想我會這樣認為
[00:32:11.280 --> 00:32:17.280]  我就從會友或者是評論的角度
[00:32:17.280 --> 00:32:19.280]  去期望一下
[00:32:19.280 --> 00:32:20.280]  教牧
[00:32:20.280 --> 00:32:23.280]  如果作為一個職場的人
[00:32:23.280 --> 00:32:25.280]  我回到教會
[00:32:25.280 --> 00:32:31.280]  我是被在我的科技或者工作裏面
[00:32:31.280 --> 00:32:33.280]  有很多的影響
[00:32:33.280 --> 00:32:37.280]  或者是我很渴望我的牧者能夠
[00:32:37.280 --> 00:32:42.280]  我不需要他知道所有科技的東西
[00:32:42.280 --> 00:32:46.280]  但是他會肯聽我說一下我的掙扎
[00:32:46.280 --> 00:32:48.280]  我面對的問題
[00:32:48.280 --> 00:32:51.280]  倫理上的一些挑戰等等
[00:32:51.280 --> 00:32:55.280]  關於科技的事情
[00:32:55.280 --> 00:32:57.280]  或者是我要去訂立一些
[00:32:57.280 --> 00:33:00.280]  我的工作是要去做一些政策
[00:33:00.280 --> 00:33:02.280]  是會影響到很多人的
[00:33:02.280 --> 00:33:04.280]  教牧在這個角色裏面
[00:33:04.280 --> 00:33:07.280]  其實是很需要同行同聆聽
[00:33:07.280 --> 00:33:10.280]  他未必會給到很多很具體的
[00:33:10.280 --> 00:33:13.280]  或者是一些很直接的東西
[00:33:13.280 --> 00:33:16.280]  但是他要明白和理解我的困難
[00:33:16.280 --> 00:33:21.280]  這個是我想會得到的一個回應
[00:33:21.280 --> 00:33:27.280]  我不是教牧
[00:33:27.280 --> 00:33:31.280]  不過我認識一個朋友是傳道人
[00:33:31.280 --> 00:33:34.280]  他跟我說最近的年輕人很灰心
[00:33:34.280 --> 00:33:37.280]  因為覺得將來AI都做了所有的東西
[00:33:37.280 --> 00:33:39.280]  我們可以做些什麼呢?
[00:33:39.280 --> 00:33:41.280]  如果面對一些
[00:33:41.280 --> 00:33:44.280]  他們有這樣的心態的時候
[00:33:44.280 --> 00:33:47.280]  當然牧者是需要想方法去回應
[00:33:47.280 --> 00:33:58.280]  好,又問了一個比較天馬行空的問題
[00:33:58.280 --> 00:33:59.280]  其實
[00:33:59.280 --> 00:34:02.280]  我回答你的問題
[00:34:02.280 --> 00:34:07.280]  三位有一天AI會不會可能被注入感情呢?
[00:34:07.280 --> 00:34:09.280]  縱然感情對AI來說
[00:34:09.280 --> 00:34:11.280]  起初是一些數據
[00:34:11.280 --> 00:34:16.280]  如果AI真的有了感情創意
[00:34:16.280 --> 00:34:20.280]  會不會叫它一個人呢?
[00:34:20.280 --> 00:34:23.280]  它有沒有靈魂呢?
[00:34:23.280 --> 00:34:26.280]  你怎樣看這個問題呢?
[00:34:26.280 --> 00:34:30.280]  如果你問
[00:34:30.280 --> 00:34:33.280]  現在我剛才所說的
[00:34:33.280 --> 00:34:38.280]  所有最近這十年來的突破性發展就是Deep Learning
[00:34:38.280 --> 00:34:40.280]  Deep Learning做到的就是Pattern Recognition
[00:34:40.280 --> 00:34:43.280]  所以如果只是靠這個突破性的發展
[00:34:43.280 --> 00:34:46.280]  我就覺得不可能有感情
[00:34:46.280 --> 00:34:48.280]  我不敢答說將來還有什麼突破
[00:34:48.280 --> 00:34:49.280]  會不會令到有感情
[00:34:49.280 --> 00:34:54.280]  但起碼在有生之年應該都不會見到有感情的AI
[00:34:54.280 --> 00:35:01.280]  我從一個基礎的角度去討論
[00:35:01.280 --> 00:35:05.280]  人的感情其實是一些
[00:35:05.280 --> 00:35:09.280]  因為我們人的腦袋有身體反應
[00:35:09.280 --> 00:35:14.280]  一些細胞裏面的化學物
[00:35:14.280 --> 00:35:17.280]  如果從學理的角度來說
[00:35:17.280 --> 00:35:21.280]  現在AI全部在電子平台下
[00:35:21.280 --> 00:35:23.280]  一同在運作
[00:35:23.280 --> 00:35:25.280]  所以
[00:35:25.280 --> 00:35:28.280]  什麼叫感情呢?
[00:35:28.280 --> 00:35:31.280]  如果我們人有一個感覺
[00:35:31.280 --> 00:35:35.280]  那種感覺其實跟AI在電腦平台是完全不同的
[00:35:35.280 --> 00:35:37.280]  如果從這樣的角度來說
[00:35:37.280 --> 00:35:39.280]  就一定沒有感情
[00:35:39.280 --> 00:35:44.280]  不過如果你說只是看表情
[00:35:44.280 --> 00:35:48.280]  機械人對答的語氣和內容
[00:35:48.280 --> 00:35:51.280]  而你將那些東西看為感情的全部
[00:35:51.280 --> 00:35:53.280]  那他可以有感情
[00:35:53.280 --> 00:35:55.280]  我剛才也有說
[00:35:55.280 --> 00:36:00.280]  其實科技是一個表面現象的重視
[00:36:00.280 --> 00:36:04.280]  所以現在都會有一些
[00:36:04.280 --> 00:36:06.280]  譬如AI輔導員
[00:36:06.280 --> 00:36:09.280]  不是說什麼
[00:36:09.280 --> 00:36:11.280]  因為有研究說
[00:36:11.280 --> 00:36:17.280]  有人對一些他知道是AI的輔導員更加坦誠地說話
[00:36:17.280 --> 00:36:20.280]  大家明白為什麼嗎?
[00:36:20.280 --> 00:36:25.280]  所以是不是要否定他能夠成為這些人的談心幫助
[00:36:25.280 --> 00:36:27.280]  這是另一個問題
[00:36:27.280 --> 00:36:29.280]  我不敢回答是或不是
[00:36:29.280 --> 00:36:31.280]  你明白嗎?
[00:36:31.280 --> 00:36:33.280]  複雜的問題
[00:36:33.280 --> 00:36:35.280]  輔導員被取代的問題
[00:36:35.280 --> 00:36:41.280]  但是人其實很容易將對著的東西
[00:36:41.280 --> 00:36:45.280]  變成以為他是人去對答
[00:36:45.280 --> 00:36:49.280]  有個詞語叫做Attribution Policy
[00:36:49.280 --> 00:36:53.280]  我們很喜歡將周圍的東西
[00:36:53.280 --> 00:36:57.280]  譬如貓、狗都會想到像人一樣
[00:36:57.280 --> 00:36:59.280]  連氣都一樣
[00:36:59.280 --> 00:37:01.280]  你真的很有空
[00:37:01.280 --> 00:37:06.280]  因為女兒喜歡看
[00:37:06.280 --> 00:37:14.280]  剛才說有一個東西叫做Social Robot
[00:37:14.280 --> 00:37:21.280]  Social Robot會做到像人有的感情跟你互動
[00:37:21.280 --> 00:37:27.280]  有些資料說這種互動叫做Asymetric感情
[00:37:27.280 --> 00:37:29.280]  即是你跟他有感情
[00:37:29.280 --> 00:37:31.280]  他可以假裝感情出來
[00:37:31.280 --> 00:37:33.280]  但其實他沒有感情
[00:37:33.280 --> 00:37:38.280]  剛才說到AI可以做到任何的模式
[00:37:38.280 --> 00:37:43.280]  當然可以假裝成一個人去表達每一種感情
[00:37:43.280 --> 00:37:46.280]  可以複雜到一個地步
[00:37:46.280 --> 00:37:49.280]  你是分辨不到一個人和一個AI的感情
[00:37:49.280 --> 00:37:51.280]  背後他是不是真的有感情呢?
[00:37:51.280 --> 00:37:54.280]  這個可以說是一個更深的哲學問題
[00:37:54.280 --> 00:37:56.280]  我們也不會再說
[00:37:56.280 --> 00:38:02.280]  我想分開感情和感官感受
[00:38:02.280 --> 00:38:04.280]  如果你去發展AI
[00:38:04.280 --> 00:38:06.280]  讓他可以有觸感
[00:38:06.280 --> 00:38:07.280]  或者是味覺
[00:38:07.280 --> 00:38:09.280]  分辨到甜酸苦辣
[00:38:09.280 --> 00:38:11.280]  我覺得是有可能的
[00:38:11.280 --> 00:38:15.280]  但是當你吃完一件東西覺得它很好吃
[00:38:15.280 --> 00:38:18.280]  我講一個自己的經歷
[00:38:18.280 --> 00:38:20.280]  每天回到中晨
[00:38:20.280 --> 00:38:24.280]  有位同工會沖一杯咖啡給我喝
[00:38:24.280 --> 00:38:27.280]  那種感情、感受
[00:38:27.280 --> 00:38:29.280]  不單純是那杯咖啡
[00:38:29.280 --> 00:38:36.280]  而是覺得被支持、被幫助
[00:38:36.280 --> 00:38:39.280]  然後你品嘗那杯咖啡的時候
[00:38:39.280 --> 00:38:41.280]  有時候我們同工在聊天
[00:38:41.280 --> 00:38:45.280]  有茶味、有什麼果味
[00:38:45.280 --> 00:38:49.280]  這些其實是一種對那件事的欣賞
[00:38:49.280 --> 00:38:51.280]  對味道的一種欣賞
[00:38:51.280 --> 00:38:57.280]  我覺得AI未必會產生到那種對味道的欣賞
[00:38:57.280 --> 00:38:59.280]  對愛、對關係的那種東西
[00:38:59.280 --> 00:39:01.280]  其實不會有
[00:39:01.280 --> 00:39:03.280]  我想補充一件事
[00:39:03.280 --> 00:39:07.280]  初期有一個電腦科學家
[00:39:07.280 --> 00:39:11.280]  寫了一個程式叫Elisa
[00:39:11.280 --> 00:39:13.280]  Elisa是一個名字
[00:39:13.280 --> 00:39:16.280]  那個程式是可以與人互動的
[00:39:16.280 --> 00:39:18.280]  秘書也有參與
[00:39:18.280 --> 00:39:21.280]  所以秘書也知道背後全部都是硬碟
[00:39:21.280 --> 00:39:24.280]  沒有真正的知識或情緒
[00:39:24.280 --> 00:39:27.280]  然後秘書就和Elisa玩耍
[00:39:27.280 --> 00:39:32.280]  那個電腦科學家就在外面等
[00:39:32.280 --> 00:39:34.280]  上次進來的時候
[00:39:34.280 --> 00:39:36.280]  秘書就趕走了她
[00:39:36.280 --> 00:39:38.280]  就說你不要妨礙我與她聊天
[00:39:38.280 --> 00:39:40.280]  因為她已經情緒上
[00:39:40.280 --> 00:39:43.280]  與她明知是硬碟的程式有關
[00:39:43.280 --> 00:39:45.280]  所以這個例子
[00:39:45.280 --> 00:39:49.280]  剛才我說的attribution policy
[00:39:49.280 --> 00:39:51.280]  又可以叫做Elisa effect
[00:39:51.280 --> 00:39:56.280]  就是人真的很喜歡自己去投射一些感情
[00:39:56.280 --> 00:39:58.280]  到一些死物身上
[00:39:58.280 --> 00:40:00.280]  我不知道這些叫不叫做idolism
[00:40:00.280 --> 00:40:04.280]  但也看到人是多麼的脆弱
[00:40:04.280 --> 00:40:06.280]  而很多時候AI的發展
[00:40:06.280 --> 00:40:09.280]  都是嘗試針對如何去欺騙人的情感
[00:40:09.280 --> 00:40:12.280]  去寫出來
[00:40:12.280 --> 00:40:15.280]  你剛才說的例子
[00:40:15.280 --> 00:40:19.280]  有另一部電影叫做HER
[00:40:19.280 --> 00:40:23.280]  說一個人愛上了那個程式
[00:40:23.280 --> 00:40:25.280]  叫什麼名字呢?
[00:40:25.280 --> 00:40:27.280]  不過相信提到這件事
[00:40:27.280 --> 00:40:29.280]  剛才我們說responsibility
[00:40:29.280 --> 00:40:31.280]  其實都是集中說
[00:40:31.280 --> 00:40:33.280]  人怎樣用AI
[00:40:33.280 --> 00:40:35.280]  以致是bring good
[00:40:35.280 --> 00:40:37.280]  不是bring harm
[00:40:37.280 --> 00:40:39.280]  但是從另一個角度來看
[00:40:39.280 --> 00:40:45.280]  人對AI本身有沒有一些倫理的obligation呢?
[00:40:45.280 --> 00:40:48.280]  舉一個極端的例子
[00:40:48.280 --> 00:40:50.280]  Westworld那套電視劇
[00:40:50.280 --> 00:40:53.280]  製造了一個robot的世界出來
[00:40:53.280 --> 00:40:56.280]  讓人可以去打那些robot
[00:40:56.280 --> 00:40:58.280]  強姦那些女robot
[00:40:58.280 --> 00:40:59.280]  沒有事情發生
[00:40:59.280 --> 00:41:01.280]  因為她是robot
[00:41:01.280 --> 00:41:05.280]  譬如有一天製造了一個Westworld
[00:41:05.280 --> 00:41:07.280]  有沒有ethical implication
[00:41:07.280 --> 00:41:10.280]  強姦一個女robot
[00:41:10.280 --> 00:41:12.280]  或者looks like a woman的robot
[00:41:12.280 --> 00:41:14.280]  是不是一種先呢?
[00:41:14.280 --> 00:41:17.280]  有沒有什麼ethical implication呢?
[00:41:17.280 --> 00:41:20.280]  如果以一個Christian的角度來看
[00:41:20.280 --> 00:41:22.280]  你看色情片都是先
[00:41:22.280 --> 00:41:25.280]  所以這樣看一定是先
[00:41:25.280 --> 00:41:29.280]  至於究竟有沒有傷害到AI
[00:41:29.280 --> 00:41:31.280]  我都不懂回答
[00:41:31.280 --> 00:41:34.280]  好像以前有一套是AI的
[00:41:34.280 --> 00:41:36.280]  有一套是自己的AI
[00:41:36.280 --> 00:41:39.280]  那套好像也有同類型的故事
[00:41:39.280 --> 00:41:42.280]  那個男孩是Hurt dealing
[00:41:42.280 --> 00:41:45.280]  也是認同的
[00:41:45.280 --> 00:41:48.280]  究竟你做的那個
[00:41:48.280 --> 00:41:50.280]  是不是有一個
[00:41:50.280 --> 00:41:54.280]  你會不會傷害到他
[00:41:54.280 --> 00:41:57.280]  什麼叫做你做AI出來
[00:41:57.280 --> 00:41:59.280]  是你傷害到他
[00:41:59.280 --> 00:42:01.280]  因為你傷害到他
[00:42:01.280 --> 00:42:06.280]  其實是假設了他有一種自我意識和感受
[00:42:06.280 --> 00:42:11.280]  當你說其實現在我們的看法
[00:42:11.280 --> 00:42:14.280]  是那些不是有自我意識和感受
[00:42:14.280 --> 00:42:16.280]  那就不會了
[00:42:16.280 --> 00:42:18.280]  但複雜的地方是
[00:42:18.280 --> 00:42:20.280]  你怎麼知道他沒有呢?
[00:42:20.280 --> 00:42:23.280]  去到那個位置就很難回答了
[00:42:23.280 --> 00:42:28.280]  我就會從人自己本身內裡
[00:42:28.280 --> 00:42:31.280]  發生什麼事的看法出現
[00:42:31.280 --> 00:42:33.280]  我們預備的時候
[00:42:33.280 --> 00:42:35.280]  聊天的時候
[00:42:35.280 --> 00:42:37.280]  都在問一個問題
[00:42:37.280 --> 00:42:39.280]  因為上帝做人
[00:42:39.280 --> 00:42:41.280]  人就做AI
[00:42:41.280 --> 00:42:45.280]  剛才阮長華說按照自己的形象去做
[00:42:45.280 --> 00:42:47.280]  上帝做人
[00:42:47.280 --> 00:42:49.280]  他是會為人而死
[00:42:49.280 --> 00:42:51.280]  耶穌基督為我們而死
[00:42:51.280 --> 00:42:53.280]  人做AI
[00:42:53.280 --> 00:42:55.280]  你會不會為AI而死
[00:42:55.280 --> 00:42:58.280]  這是一個對人的挑戰
[00:42:58.280 --> 00:43:00.280]  去到某一天你會為AI而死
[00:43:00.280 --> 00:43:04.280]  你就跟他有一種關係在裡面
[00:43:04.280 --> 00:43:06.280]  我未必能回答這個問題
[00:43:06.280 --> 00:43:11.280]  但是人自己本身那種道德責任
[00:43:11.280 --> 00:43:13.280]  回到最後都要問自己
[00:43:13.280 --> 00:43:15.280]  我視這件事是什麼
[00:43:15.280 --> 00:43:20.280]  是一個工具還是我跟他有一個愛的關係
[00:43:20.280 --> 00:43:22.280]  會不會去到一個這樣的地步
[00:43:22.280 --> 00:43:26.280]  這是一個要繼續問下去的問題
[00:43:26.280 --> 00:43:28.280]  我不知道小蕾老師有沒有什麼回應
[00:43:28.280 --> 00:43:32.280]  沒有,我只想Westworld的電視劇
[00:43:32.280 --> 00:43:35.280]  終於有一個人想問問題
[00:43:35.280 --> 00:43:37.280]  你會是第一個拿咪問
[00:43:37.280 --> 00:43:39.280]  也是最後一個問問題
[00:43:39.280 --> 00:43:41.280]  時間到了
[00:43:41.280 --> 00:43:49.280]  多謝三位講員的分享
[00:43:49.280 --> 00:43:53.280]  都在豐富我們對於新世界的想像
[00:43:53.280 --> 00:43:57.280]  不過我也想問一個問題
[00:43:57.280 --> 00:43:59.280]  可能也切身關注
[00:43:59.280 --> 00:44:03.280]  當AI科技一直發展下去的時候
[00:44:03.280 --> 00:44:08.280]  其實不只是我們普通人用
[00:44:08.280 --> 00:44:13.280]  其實牽涉到商界甚至是政權的使用
[00:44:13.280 --> 00:44:16.280]  剛才說到一件事
[00:44:16.280 --> 00:44:18.280]  舉一個例子
[00:44:18.280 --> 00:44:21.280]  可能在要用一些科技的時候
[00:44:21.280 --> 00:44:23.280]  有些老人家用不了
[00:44:23.280 --> 00:44:27.280]  但是這個科技是用來監控人的
[00:44:27.280 --> 00:44:31.280]  是用來塑造人的本性、本質
[00:44:31.280 --> 00:44:37.280]  作為教會究竟是要達到這個要求
[00:44:37.280 --> 00:44:41.280]  去做這件事還是維持穩定
[00:44:41.280 --> 00:44:44.280]  因為這件事可能是幫助人活得更加舒服
[00:44:44.280 --> 00:44:46.280]  我們正在進入這個世界
[00:44:46.280 --> 00:44:48.280]  我們說要幫助人進入世界
[00:44:48.280 --> 00:44:50.280]  但同一時間我們又要抗拒這件事
[00:44:50.280 --> 00:44:53.280]  究竟教會應該怎樣回應這件事呢?
[00:44:53.280 --> 00:44:56.280]  怎樣回應使用科技上呢?
[00:44:56.280 --> 00:44:58.280]  我想這是幾方面的問題
[00:44:58.280 --> 00:45:00.280]  多謝
[00:45:00.280 --> 00:45:07.280]  我先試一下回答
[00:45:07.280 --> 00:45:11.280]  剛才你說得很好
[00:45:11.280 --> 00:45:16.280]  一方面我們在整個現代社會
[00:45:16.280 --> 00:45:21.280]  你不進入科技的社會
[00:45:21.280 --> 00:45:24.280]  你就會被淘汰
[00:45:24.280 --> 00:45:26.280]  差不多是必然地
[00:45:26.280 --> 00:45:30.280]  你不玩了,去深山自己住
[00:45:30.280 --> 00:45:36.280]  不然你就一定要去學或做科技的東西
[00:45:36.280 --> 00:45:38.280]  你才不會被淘汰
[00:45:38.280 --> 00:45:40.280]  舉個例子
[00:45:40.280 --> 00:45:42.280]  你用查GPT
[00:45:42.280 --> 00:45:45.280]  你當所有其他同學都懂得用查GPT去找答案
[00:45:45.280 --> 00:45:48.280]  但你不懂的,你當然會慢很多
[00:45:48.280 --> 00:45:51.280]  然後你就會慢慢被淘汰
[00:45:51.280 --> 00:45:53.280]  如果這是一個遊戲規則
[00:45:53.280 --> 00:45:58.280]  然後,我忘記了之後的問題
[00:45:58.280 --> 00:46:04.280]  如果沒有科技,那怎樣?
[00:46:05.280 --> 00:46:09.280]  即使是雙人,同一系統的科技
[00:46:09.280 --> 00:46:13.280]  你包括一些職業科技的人
[00:46:13.280 --> 00:46:16.280]  這是一個牧師的問題
[00:46:16.280 --> 00:46:18.280]  多謝
[00:46:18.280 --> 00:46:24.280]  很難回答
[00:46:24.280 --> 00:46:29.280]  因為問題的弟兄其實是說了兩者
[00:46:29.280 --> 00:46:34.280]  都不會有一個絕對偏一邊
[00:46:34.280 --> 00:46:40.280]  我會說一個甚麼的案件
[00:46:40.280 --> 00:46:44.280]  很難以片蓋全地說
[00:46:44.280 --> 00:46:47.280]  你可以怎樣
[00:46:47.280 --> 00:46:54.280]  當然我們不應該去幫Totalitarianism
[00:46:54.280 --> 00:46:58.280]  但其實甚麼才是Totalitarianism呢?
[00:46:58.280 --> 00:47:03.280]  但你初時Google Map幫你開車
[00:47:03.280 --> 00:47:05.280]  是不是Totalitarianism呢?
[00:47:05.280 --> 00:47:07.280]  你明白嗎?
[00:47:07.280 --> 00:47:09.280]  誰不用呢?
[00:47:09.280 --> 00:47:11.280]  很方便
[00:47:11.280 --> 00:47:13.280]  老鬼不喜歡用
[00:47:13.280 --> 00:47:15.280]  看了地圖,但現在地圖很難買
[00:47:15.280 --> 00:47:21.280]  你明白嗎?其實你都要收很多資料
[00:47:21.280 --> 00:47:25.280]  但原初發展出來,其實真的很有用
[00:47:25.280 --> 00:47:29.280]  你跟著說你給了誰不良資料去用
[00:47:29.280 --> 00:47:31.280]  但又真的可以
[00:47:31.280 --> 00:47:33.280]  那怎樣呢?
[00:47:33.280 --> 00:47:36.280]  其實是很複雜的
[00:47:36.280 --> 00:47:40.280]  如果真的要這樣回應
[00:47:40.280 --> 00:47:46.280]  是要看一個很實質的,究竟在說甚麼事
[00:47:46.280 --> 00:47:48.280]  發生了甚麼事
[00:47:48.280 --> 00:47:52.280]  在裏面可能做到某些決定
[00:47:52.280 --> 00:47:57.280]  最後一個回應是比較好,因為有很多時間去想
[00:47:57.280 --> 00:48:01.280]  我會看教會是甚麼呢?
[00:48:01.280 --> 00:48:06.280]  教會不只是堂會,不是說牧師怎樣回應
[00:48:06.280 --> 00:48:11.280]  而是教會是說整個基督徒群體如何去回應
[00:48:11.280 --> 00:48:13.280]  如果這樣角度去看
[00:48:13.280 --> 00:48:18.280]  你會發現每一個個體在不同的崗位裏
[00:48:18.280 --> 00:48:22.280]  每一個人都可以有自己在崗位裏的回應
[00:48:22.280 --> 00:48:24.280]  如果你是一個牧師
[00:48:24.280 --> 00:48:30.280]  你會去鼓勵信徒,回應信徒的品格能力
[00:48:30.280 --> 00:48:32.280]  去回應他自己用科技
[00:48:32.280 --> 00:48:35.280]  如果你是一個基督徒
[00:48:35.280 --> 00:48:42.280]  你是在做管理,或者制度上的事
[00:48:42.280 --> 00:48:45.280]  你就要問自己,在那一刻裏
[00:48:45.280 --> 00:48:50.280]  聖經對你來說,你要做一些規範和政策的時候
[00:48:50.280 --> 00:48:52.280]  你是一個怎樣的人呢?
[00:48:52.280 --> 00:48:55.280]  當然,如果你是一個技術很強的人
[00:48:55.280 --> 00:48:59.280]  你可以幫助教會去使用這些科技
[00:48:59.280 --> 00:49:04.280]  以至無論宣教,教會的運作,幫助一些困難的人等等
[00:49:04.280 --> 00:49:08.280]  每一個人都應該有自己的角色在裏面
[00:49:08.280 --> 00:49:10.280]  這才是教會的正題
[00:49:10.280 --> 00:49:13.280]  宗神說的是collaboration(合作)
[00:49:13.280 --> 00:49:17.280]  所以那一刻,我們怎樣可以從一個整體的角度
[00:49:17.280 --> 00:49:22.280]  去回應這麼大的挑戰
[00:49:22.280 --> 00:49:26.280]  這是一個共同的研習課題
[00:49:26.280 --> 00:49:32.280]  不知道宗神的同學,遲些會做一個capstone的論文
[00:49:32.280 --> 00:49:34.280]  去講一下就最好了
[00:49:34.280 --> 00:49:42.280]  多謝Alex用每個人要做反思協作者去回應這個議題的定義
[00:49:42.280 --> 00:49:45.280]  再次多謝三位講員
[00:49:45.280 --> 00:49:53.280]  最後請院長做一個結束的禱告
[00:49:53.280 --> 00:49:55.280]  不如我們一起祈禱
[00:49:55.280 --> 00:50:06.280]  天父,我們知道你創造我們
[00:50:06.280 --> 00:50:09.280]  你將我們放在時代裏面
[00:50:09.280 --> 00:50:12.280]  亦都出現AI這樣的事情
[00:50:12.280 --> 00:50:15.280]  天父,我們見到AI都是你給我們
[00:50:15.280 --> 00:50:19.280]  能夠去彰顯到人類的智慧
[00:50:19.280 --> 00:50:23.280]  又或者在很多地方都能夠幫助我們做很多事情
[00:50:23.280 --> 00:50:26.280]  不過同時我們都見到AI給我們很多的影響
[00:50:26.280 --> 00:50:28.280]  我們都要去到小心
[00:50:28.280 --> 00:50:31.280]  天父,首先我們來到你的面前
[00:50:31.280 --> 00:50:34.280]  我們都要承認,都要謙虛地去到學習
[00:50:34.280 --> 00:50:39.280]  讓我們都真是懂得如何辨識今天的世代
[00:50:39.280 --> 00:50:42.280]  在今天的世代裏面作你忠心的僕人
[00:50:42.280 --> 00:50:45.280]  多謝你給我們今晚一些相聚的時間
[00:50:45.280 --> 00:50:49.280]  我們不敢說有很深的道理能夠出來
[00:50:49.280 --> 00:50:53.280]  不過我們希望今天的訊息能夠幫助到弟兄姊妹
[00:50:53.280 --> 00:50:56.280]  我們認真面對這個課題
[00:50:56.280 --> 00:50:58.280]  懂得開始去思考
[00:50:58.280 --> 00:51:02.280]  亦都懂得去留意AI給我們的影響
[00:51:02.280 --> 00:51:06.280]  求你繼續幫助我們往後的日子
[00:51:06.280 --> 00:51:10.280]  我們有空間有機會更加多去思想
[00:51:10.280 --> 00:51:14.280]  亦都更加多去善用你給我們的恩賜
[00:51:14.280 --> 00:51:18.280]  亦都小心這個世界給我們的引誘和陷阱
[00:51:18.280 --> 00:51:20.280]  所以求你這樣聽我們的祈禱
[00:51:20.280 --> 00:51:22.280]  祈禱供奉耶穌,記得我們的名字祈求,阿門
[00:51:22.280 --> 00:51:25.280]  多謝大家,今天就到此為止
[00:51:26.280 --> 00:51:37.280]  (音樂)


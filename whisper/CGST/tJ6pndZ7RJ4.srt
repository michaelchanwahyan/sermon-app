1
00:00:00,000 --> 00:00:07,600
(音樂)

2
00:00:07,600 --> 00:00:10,640
按時間都會問大家的

3
00:00:10,640 --> 00:00:13,440
不過我做主席當然要兇一點

4
00:00:13,440 --> 00:00:16,640
首先問一個問題

5
00:00:16,640 --> 00:00:20,080
先問一下Felix一個很簡單的問題

6
00:00:20,080 --> 00:00:23,200
最近有沒有問checkGPT應該買哪隻股票

7
00:00:23,200 --> 00:00:25,200
沒有

8
00:00:25,200 --> 00:00:28,080
我不是很敢買股票的

9
00:00:28,080 --> 00:00:31,680
我剛才有幅圖

10
00:00:31,680 --> 00:00:33,680
我不知道是誰找回來的

11
00:00:33,680 --> 00:00:39,200
他買股票的能力好像比Ron Buffett好

12
00:00:39,200 --> 00:00:41,200
我不知道是真是假

13
00:00:41,200 --> 00:00:43,200
我不會看內容

14
00:00:43,200 --> 00:00:47,760
問一個比較具體的問題

15
00:00:47,760 --> 00:00:51,760
我發覺很多人都問院長

16
00:00:51,760 --> 00:00:56,240
可能你講了一些很偉大的事情

17
00:00:56,240 --> 00:00:58,240
我先問一下你的偉大事情

18
00:00:58,240 --> 00:01:01,360
你提到AI主要有兩個危機

19
00:01:01,360 --> 00:01:03,360
一個是Singularity

20
00:01:03,360 --> 00:01:05,360
AI聰明到一個地步

21
00:01:05,360 --> 00:01:08,240
可以像Terminator一樣毀滅人類

22
00:01:08,240 --> 00:01:10,720
不過我覺得如果AI去到一個地步

23
00:01:10,720 --> 00:01:12,720
我都沒得救了

24
00:01:12,720 --> 00:01:16,080
我只希望下一個突破是100年之後

25
00:01:16,080 --> 00:01:18,080
不是我一生的了

26
00:01:18,080 --> 00:01:20,080
但你講到另一個問題

27
00:01:20,080 --> 00:01:22,640
Digital Totalitarianism

28
00:01:22,640 --> 00:01:25,280
這個似乎就近很多了

29
00:01:25,280 --> 00:01:27,280
你講到Student

30
00:01:27,280 --> 00:01:29,280
那些都好像是正在發生的

31
00:01:29,280 --> 00:01:31,280
我想問一下作為一個

32
00:01:31,280 --> 00:01:33,280
Educated Citizen

33
00:01:33,280 --> 00:01:35,280
一個有教育的公民

34
00:01:35,280 --> 00:01:39,280
其實在社會裏面

35
00:01:39,280 --> 00:01:41,280
我作為一個公民

36
00:01:41,280 --> 00:01:43,280
有什麼可以做

37
00:01:43,280 --> 00:01:45,280
去面對或者嘗試

38
00:01:45,280 --> 00:01:49,280
等這一天不會來臨呢?

39
00:01:49,280 --> 00:01:53,280
通常我們在

40
00:01:53,280 --> 00:01:55,280
剛才說的Philosophy of Technology

41
00:01:55,280 --> 00:02:01,280
我們對於如何面對科技的操控

42
00:02:01,280 --> 00:02:05,280
有兩個角度去看

43
00:02:05,280 --> 00:02:09,280
第一個就是Spiritual的角度

44
00:02:09,280 --> 00:02:11,280
一個人如何面對的角度

45
00:02:11,280 --> 00:02:15,280
第二個就是科技本身如何去發展的角度

46
00:02:15,280 --> 00:02:19,280
剛才你問過應該是屬於人如何面對的角度

47
00:02:19,280 --> 00:02:23,280
我會想起

48
00:02:23,280 --> 00:02:27,280
我老婆很喜歡看YouTube

49
00:02:27,280 --> 00:02:29,280
她推薦什麼都看

50
00:02:29,280 --> 00:02:33,280
我就會叫她不要別人推薦你就看

51
00:02:33,280 --> 00:02:35,280
你寧願自己搜尋一些你想看的

52
00:02:35,280 --> 00:02:37,280
因為她推薦你的東西

53
00:02:37,280 --> 00:02:40,280
其實她已經在收集你的資料

54
00:02:40,280 --> 00:02:42,280
而操控著你

55
00:02:42,280 --> 00:02:47,280
你沒有理由讓她自由地去操控你

56
00:02:47,280 --> 00:02:49,280
其中一件事就是

57
00:02:49,280 --> 00:02:53,280
當你正在使用科技產品的時候

58
00:02:53,280 --> 00:02:56,280
自己也要在意他們背後在做的事

59
00:02:56,280 --> 00:02:59,280
如果你在意他們背後在做的事的時候

60
00:02:59,280 --> 00:03:03,280
你也會嘗試採用一些方法

61
00:03:03,280 --> 00:03:05,280
令自己不會被他們影響

62
00:03:05,280 --> 00:03:09,280
就像Netflix

63
00:03:09,280 --> 00:03:11,280
我老婆也是

64
00:03:11,280 --> 00:03:13,280
Facebook現在的短片

65
00:03:13,280 --> 00:03:15,280
一個連一個

66
00:03:15,280 --> 00:03:17,280
我老婆就是這樣

67
00:03:17,280 --> 00:03:21,280
不過你這個問題引申我又想繼續問你

68
00:03:21,280 --> 00:03:23,280
也想問一下Alex

69
00:03:23,280 --> 00:03:25,280
Alex剛才提到

70
00:03:25,280 --> 00:03:29,280
所謂做一個Virtuous person去用科技

71
00:03:29,280 --> 00:03:31,280
剛才Felix也舉了一個例子

72
00:03:31,280 --> 00:03:34,280
他說你跟著他去做

73
00:03:34,280 --> 00:03:37,280
可能就不是最Virtuous

74
00:03:37,280 --> 00:03:39,280
起碼不是最聰明的做法

75
00:03:39,280 --> 00:03:43,280
其實有沒有一些比較具體的例子

76
00:03:43,280 --> 00:03:46,280
例如你剛才提到的Develop Education

77
00:03:46,280 --> 00:03:47,280
不是我做的

78
00:03:47,280 --> 00:03:48,280
或者是你做到的

79
00:03:48,280 --> 00:03:49,280
不是我做到的

80
00:03:49,280 --> 00:03:52,280
我們是普通的普通的用戶

81
00:03:52,280 --> 00:03:53,280
怎樣可以做一個

82
00:03:53,280 --> 00:03:57,280
所謂更加Virtuous去用科技的人呢?

83
00:03:57,280 --> 00:03:59,280
Felix或是Alex

84
00:03:59,280 --> 00:04:00,280
補充一下

85
00:04:00,280 --> 00:04:02,280
我經常覺得

86
00:04:02,280 --> 00:04:05,280
你首先要做一個Virtuous的人

87
00:04:05,280 --> 00:04:07,280
在這個世代裏

88
00:04:07,280 --> 00:04:09,280
你首先要讓自己成為

89
00:04:09,280 --> 00:04:11,280
你要知道他發生什麼事

90
00:04:11,280 --> 00:04:14,280
其實一個Literary的人

91
00:04:14,280 --> 00:04:17,280
你要做一個有Virtual的人

92
00:04:17,280 --> 00:04:21,280
首先你要成為一個Literary的人

93
00:04:21,280 --> 00:04:25,280
你要明白背後的運作

94
00:04:25,280 --> 00:04:26,280
或者是認識

95
00:04:26,280 --> 00:04:27,280
例如聽講座

96
00:04:27,280 --> 00:04:30,280
幫助你去知道更多

97
00:04:30,280 --> 00:04:33,280
其實他背後在發生什麼事

98
00:04:33,280 --> 00:04:34,280
當然剛才我說到

99
00:04:34,280 --> 00:04:37,280
未必是要你去上一個課程

100
00:04:37,280 --> 00:04:40,280
但是很多的文章或者是一些反省

101
00:04:40,280 --> 00:04:43,280
一直告訴我們

102
00:04:43,280 --> 00:04:46,280
這些科技背後

103
00:04:46,280 --> 00:04:50,280
其實他們是在用什麼價值觀等等

104
00:04:50,280 --> 00:04:54,280
當你有對科技的認識

105
00:04:54,280 --> 00:04:58,280
你明白它的限制和邏輯的時候

106
00:04:58,280 --> 00:05:02,280
你就會開始慢慢產生一種判斷的能力

107
00:05:02,280 --> 00:05:04,280
一個Determine的能力

108
00:05:04,280 --> 00:05:07,280
所以這是一個過程

109
00:05:07,280 --> 00:05:09,280
或者可以這樣說

110
00:05:09,280 --> 00:05:11,280
這個世代是很喜歡

111
00:05:11,280 --> 00:05:13,280
這個數碼文化

112
00:05:13,280 --> 00:05:14,280
很喜歡Feed東西給你

113
00:05:14,280 --> 00:05:15,280
剛才我們一直說到

114
00:05:15,280 --> 00:05:19,280
用它的方法去Feed一些Data

115
00:05:19,280 --> 00:05:20,280
或者Feed一些內容給你

116
00:05:20,280 --> 00:05:23,280
我們其實是很習慣有一種

117
00:05:23,280 --> 00:05:25,280
一看就知道

118
00:05:25,280 --> 00:05:26,280
那種文化

119
00:05:26,280 --> 00:05:27,280
坦白說,我也要承認

120
00:05:27,280 --> 00:05:30,280
我有時候自己也會很累的時候

121
00:05:30,280 --> 00:05:33,280
你送什麼給我,我就看什麼

122
00:05:33,280 --> 00:05:35,280
那是一個問題

123
00:05:35,280 --> 00:05:38,280
我們要有一種很大的Awareness

124
00:05:38,280 --> 00:05:43,280
Awareness是一個重要的元素

125
00:05:43,280 --> 00:05:49,280
你要面對他們那種Feed給你的東西

126
00:05:49,280 --> 00:05:51,280
你要主動地去抗衡

127
00:05:51,280 --> 00:05:53,280
我暫時就說到這裡

128
00:05:53,280 --> 00:05:54,280
不知道兩位有沒有其他補充

129
00:05:54,280 --> 00:05:56,280
最後一句就補充一下

130
00:05:56,280 --> 00:05:59,280
Virtual或者美德一定是操練

131
00:05:59,280 --> 00:06:01,280
而操練當然是不舒服

132
00:06:01,280 --> 00:06:03,280
就好像你去跑步

133
00:06:03,280 --> 00:06:04,280
當然是不舒服

134
00:06:04,280 --> 00:06:06,280
就是因為科技或者AI

135
00:06:06,280 --> 00:06:07,280
Feed給你的東西

136
00:06:07,280 --> 00:06:09,280
你看看很開心,很暢順

137
00:06:09,280 --> 00:06:10,280
又不用想太多

138
00:06:10,280 --> 00:06:13,280
譬如Felix提他老婆的例子也是

139
00:06:13,280 --> 00:06:16,280
因為很舒服,很適合看

140
00:06:16,280 --> 00:06:18,280
那你就會做

141
00:06:18,280 --> 00:06:21,280
其實科技特色就是讓你很舒服

142
00:06:21,280 --> 00:06:26,280
在這裡你真的要決定成為一個有美德的人

143
00:06:26,280 --> 00:06:28,280
你要知道其實你是抗衡一些

144
00:06:28,280 --> 00:06:31,280
你未必很暢順很舒服的東西

145
00:06:31,280 --> 00:06:33,280
所以剛才說的時候

146
00:06:33,280 --> 00:06:35,280
我也會想起屬靈操練

147
00:06:35,280 --> 00:06:37,280
以前我們屬靈操練說的

148
00:06:37,280 --> 00:06:39,280
譬如我們看

149
00:06:39,280 --> 00:06:40,280
我來的年代

150
00:06:40,280 --> 00:06:41,280
看Richard Forster

151
00:06:41,280 --> 00:06:44,280
說怎樣去靜

152
00:06:44,280 --> 00:06:46,280
怎樣去不想事

153
00:06:46,280 --> 00:06:49,280
怎樣去不被挑釁影響自己

154
00:06:49,280 --> 00:06:51,280
而其實同一個操練

155
00:06:51,280 --> 00:06:54,280
是可以在這個情況下

156
00:06:54,280 --> 00:06:56,280
令自己可以有控制

157
00:06:56,280 --> 00:07:01,280
現在其實是公開問問題的時間

158
00:07:01,280 --> 00:07:07,280
我會繼續選一些已經在WhatsApp群組提出的問題

159
00:07:07,280 --> 00:07:09,280
除了這個之外

160
00:07:09,280 --> 00:07:11,280
中間會有咪高峰

161
00:07:11,280 --> 00:07:15,280
如果你想直接問的話

162
00:07:15,280 --> 00:07:17,280
更加生動的話

163
00:07:17,280 --> 00:07:20,280
請你走到咪高峰前面

164
00:07:20,280 --> 00:07:21,280
先不要說話

165
00:07:21,280 --> 00:07:23,280
我知道原來你想問

166
00:07:23,280 --> 00:07:25,280
可能等回答完

167
00:07:25,280 --> 00:07:27,280
我看到就會請你發言

168
00:07:27,280 --> 00:07:30,280
所以如果你想走出來問

169
00:07:30,280 --> 00:07:32,280
現在可以走出來

170
00:07:32,280 --> 00:07:34,280
否則也可以繼續WhatsApp問

171
00:07:34,280 --> 00:07:38,280
不過首先問院長

172
00:07:38,280 --> 00:07:42,280
其實也延續剛才的操練問題

173
00:07:42,280 --> 00:07:46,280
你提到我們要做科技的主人

174
00:07:46,280 --> 00:07:49,280
不要科技做了我們的主人

175
00:07:49,280 --> 00:07:51,280
怎樣才叫科技做了我們的主人呢

176
00:07:51,280 --> 00:07:52,280
舉個例子

177
00:07:52,280 --> 00:07:54,280
最近中晨

178
00:07:54,280 --> 00:07:57,280
我的辦公室的冷氣機壞了

179
00:07:57,280 --> 00:08:00,280
所以我在家工作

180
00:08:00,280 --> 00:08:04,280
幫拓展部說中晨很多設施都很舊了

181
00:08:04,280 --> 00:08:06,280
所以你記不記得

182
00:08:06,280 --> 00:08:10,280
中晨我的辦公室的冷氣機壞了

183
00:08:10,280 --> 00:08:12,280
所以我就留在家工作

184
00:08:12,280 --> 00:08:14,280
因為家裡的冷氣機沒有壞

185
00:08:14,280 --> 00:08:17,280
我是否依靠冷氣呢

186
00:08:17,280 --> 00:08:21,280
我是否用科技讓我可以活得更加好

187
00:08:21,280 --> 00:08:26,280
我已經好像成為了科技的奴隸

188
00:08:26,280 --> 00:08:28,280
沒有它我就死定了

189
00:08:28,280 --> 00:08:33,280
可能呼籲一下奉獻給你房間的冷氣機

190
00:08:33,280 --> 00:08:40,280
其實很難就這樣說什麼是什麼不是

191
00:08:40,280 --> 00:08:42,280
尤其是操練那裡

192
00:08:42,280 --> 00:08:44,280
其實如果你的領袖是不開冷氣

193
00:08:44,280 --> 00:08:46,280
我們知道現在有位香港的名人

194
00:08:46,280 --> 00:08:48,280
付電費那樣

195
00:08:48,280 --> 00:08:51,280
其實我很欣賞

196
00:08:51,280 --> 00:08:53,280
其實很有趣

197
00:08:53,280 --> 00:08:54,280
社會笑他

198
00:08:54,280 --> 00:08:57,280
他動機在各方面

199
00:08:57,280 --> 00:08:59,280
但問題就是這樣

200
00:08:59,280 --> 00:09:05,280
困難的地方就是科技真的不是本身邪惡

201
00:09:05,280 --> 00:09:08,280
但是因為我們用了它

202
00:09:08,280 --> 00:09:11,280
我們就慢慢靠它

203
00:09:11,280 --> 00:09:14,280
你說你不靠它就不行

204
00:09:14,280 --> 00:09:19,280
因為就算創世的第二章耕種也是科技

205
00:09:19,280 --> 00:09:24,280
難道以前的人打獵也是

206
00:09:24,280 --> 00:09:26,280
不過你去到什麼程度而已

207
00:09:26,280 --> 00:09:28,280
所以這些事情

208
00:09:28,280 --> 00:09:31,280
我想有很多因素在背後決定

209
00:09:31,280 --> 00:09:33,280
你個人的領袖

210
00:09:33,280 --> 00:09:35,280
你看看大衛的環境

211
00:09:35,280 --> 00:09:40,280
說真的,你也知道鍾成日老師到今天仍然在開槍

212
00:09:40,280 --> 00:09:44,280
真的有這樣的人

213
00:09:44,280 --> 00:09:48,280
我想就是一種你怎樣操練

214
00:09:48,280 --> 00:09:50,280
還有有趣的地方

215
00:09:50,280 --> 00:09:52,280
人身體有反應

216
00:09:52,280 --> 00:09:54,280
你不開冷氣太多

217
00:09:54,280 --> 00:09:56,280
其實你會習慣

218
00:09:56,280 --> 00:09:58,280
其實是慢慢習慣

219
00:09:58,280 --> 00:10:00,280
因為有在我們不習慣

220
00:10:00,280 --> 00:10:02,280
慢慢我們就覺得不行

221
00:10:02,280 --> 00:10:05,280
這是科技帶領我們走的路

222
00:10:05,280 --> 00:10:07,280
一定要

223
00:10:07,280 --> 00:10:09,280
是不是一定要呢

224
00:10:09,280 --> 00:10:12,280
我經常說創世第一章

225
00:10:12,280 --> 00:10:14,280
甚好的世界而是沒有科技

226
00:10:14,280 --> 00:10:16,280
為什麼今天要呢

227
00:10:16,280 --> 00:10:20,280
這個就是我們經常問的問題

228
00:10:20,280 --> 00:10:24,280
下次行山不要帶蚊子爬水

229
00:10:24,280 --> 00:10:27,280
那就打了針

230
00:10:27,280 --> 00:10:28,280
小心

231
00:10:28,280 --> 00:10:30,280
我想說一點

232
00:10:30,280 --> 00:10:32,280
其實你想一下我們現在坐在這裡

233
00:10:32,280 --> 00:10:34,280
我們的咪高峰

234
00:10:34,280 --> 00:10:35,280
桌子

235
00:10:35,280 --> 00:10:36,280
椅子

236
00:10:36,280 --> 00:10:38,280
其實所有這些東西都是科技

237
00:10:38,280 --> 00:10:41,280
我們就在這個科技的環境生活

238
00:10:41,280 --> 00:10:46,280
問題就是什麼時候科技會令我們沒有空間

239
00:10:46,280 --> 00:10:50,280
去做我們想做的事

240
00:10:50,280 --> 00:10:52,280
如果沒有了就是問題

241
00:10:52,280 --> 00:10:55,280
這是科技整個發展的問題

242
00:10:55,280 --> 00:11:01,280
有時候普通人未必有那個位置去改變

243
00:11:01,280 --> 00:11:05,280
但如果我們希望它繼續發展出來

244
00:11:05,280 --> 00:11:10,280
就有空間讓大家繼續發揮做人的本性

245
00:11:10,280 --> 00:11:14,280
有一個問題

246
00:11:14,280 --> 00:11:16,280
今天有問

247
00:11:16,280 --> 00:11:19,280
我想不少基督徒在網上都問過

248
00:11:19,280 --> 00:11:24,280
現在check gbd可以整篇文章寫出來

249
00:11:24,280 --> 00:11:26,280
可不可以問check gbd

250
00:11:26,280 --> 00:11:30,280
根據馬太福音三宗十七節應該怎樣講道

251
00:11:30,280 --> 00:11:33,280
如果check gbd弄了篇講道出來

252
00:11:33,280 --> 00:11:36,280
到底有沒有聖靈帶領的呢

253
00:11:36,280 --> 00:11:37,280
AI

254
00:11:37,280 --> 00:11:40,280
我可不可以先回答

255
00:11:40,280 --> 00:11:42,280
我試過聽完篇講道

256
00:11:42,280 --> 00:11:45,280
然後我問check gbd

257
00:11:45,280 --> 00:11:47,280
叫他講同一個話題

258
00:11:47,280 --> 00:11:48,280
其實是很廢的

259
00:11:48,280 --> 00:11:50,280
所以暫時來說

260
00:11:50,280 --> 00:11:53,280
牧師的工作是安全的

261
00:11:59,280 --> 00:12:03,280
我想大家可能都會八卦

262
00:12:03,280 --> 00:12:06,280
或者用實驗性質試過打一些東西

263
00:12:06,280 --> 00:12:10,280
然後問他做哪裏等等

264
00:12:10,280 --> 00:12:13,280
我想一開始你會覺得很神奇

265
00:12:13,280 --> 00:12:14,280
為甚麼可以做到

266
00:12:14,280 --> 00:12:17,280
但當你細心讀他的內容的時候

267
00:12:17,280 --> 00:12:22,280
你會發現對我的生命有甚麼幫助

268
00:12:22,280 --> 00:12:27,280
真的很普通

269
00:12:27,280 --> 00:12:30,280
講道也好,或者是悉經也好

270
00:12:30,280 --> 00:12:34,280
其實很多時候是講講者的生命

271
00:12:34,280 --> 00:12:36,280
他自己的經歷

272
00:12:36,280 --> 00:12:38,280
當他跟你講的時候

273
00:12:38,280 --> 00:12:41,280
那個訊息是他的生命與你的對話

274
00:12:41,280 --> 00:12:44,280
那個才是對我們最大的影響

275
00:12:44,280 --> 00:12:45,280
對我來說是

276
00:12:45,280 --> 00:12:48,280
如果有一天有人去舉辦

277
00:12:48,280 --> 00:12:50,280
香港舉辦一個AI的崇拜

278
00:12:50,280 --> 00:12:52,280
我會參加

279
00:12:52,280 --> 00:12:54,280
我會看看你在做甚麼

280
00:12:54,280 --> 00:12:59,280
我不會覺得在當中會有甚麼生命被祝福

281
00:12:59,280 --> 00:13:02,280
其實問題有很多層次

282
00:13:02,280 --> 00:13:08,280
如果用AI來幫你做一些基本的資料搜集

283
00:13:08,280 --> 00:13:09,280
我覺得是很…

284
00:13:09,280 --> 00:13:11,280
你要知道它是否準確

285
00:13:11,280 --> 00:13:13,280
我曾經問過AI

286
00:13:13,280 --> 00:13:15,280
問他一些關於創世紀的人物

287
00:13:15,280 --> 00:13:17,280
他給了我摩西

288
00:13:17,280 --> 00:13:20,280
無論如何,很低級的錯誤都會發生

289
00:13:20,280 --> 00:13:21,280
當然隨著年日

290
00:13:21,280 --> 00:13:23,280
他應該會有些進步

291
00:13:23,280 --> 00:13:24,280
我的意思是

292
00:13:24,280 --> 00:13:26,280
即使你今天問他也要很小心

293
00:13:26,280 --> 00:13:28,280
不要完全相信他

294
00:13:28,280 --> 00:13:31,280
你說預備港島

295
00:13:31,280 --> 00:13:33,280
其實有基本的資料告訴你

296
00:13:33,280 --> 00:13:35,280
某些聖經背景

297
00:13:35,280 --> 00:13:36,280
你查過方便

298
00:13:36,280 --> 00:13:37,280
其實是可以的

299
00:13:37,280 --> 00:13:39,280
你用悉經書也是這樣

300
00:13:39,280 --> 00:13:41,280
不過預備訊息

301
00:13:41,280 --> 00:13:42,280
我們都知道

302
00:13:42,280 --> 00:13:46,280
預備訊息其實我們有很多悉經書的工具

303
00:13:46,280 --> 00:13:49,280
不過悉經書是幫不到你真正的訊息出現

304
00:13:49,280 --> 00:13:51,280
如果有同工或聽港島的

305
00:13:51,280 --> 00:13:52,280
你才會知道

306
00:13:52,280 --> 00:13:54,280
那是一種生命的東西

307
00:13:54,280 --> 00:13:56,280
預備就沒有問題

308
00:13:56,280 --> 00:13:59,280
你說是否有一個AI的機械人在港島

309
00:13:59,280 --> 00:14:00,280
是不可以的

310
00:14:00,280 --> 00:14:02,280
有一個例子就是

311
00:14:02,280 --> 00:14:04,280
保羅在提瑪撒前說

312
00:14:04,280 --> 00:14:05,280
「我是一個罪魁」

313
00:14:05,280 --> 00:14:07,280
機械人怎麼做罪魁呢?

314
00:14:07,280 --> 00:14:09,280
那是一個生命

315
00:14:09,280 --> 00:14:12,280
那是上帝跟傳道人所說的話

316
00:14:12,280 --> 00:14:16,280
讓他生命跟上帝去接觸

317
00:14:16,280 --> 00:14:17,280
聆聽

318
00:14:17,280 --> 00:14:20,280
然後透過他生命再說給那個會眾

319
00:14:20,280 --> 00:14:22,280
這是傳道人的工作

320
00:14:22,280 --> 00:14:26,280
即使那個技巧或文章開設得好

321
00:14:26,280 --> 00:14:29,280
這是目者的訊息說到

322
00:14:29,280 --> 00:14:31,280
其實我的看法是不能被代替

323
00:14:31,280 --> 00:14:41,280
如果你看回GPT或Deep Learning做到的東西

324
00:14:41,280 --> 00:14:43,280
就是Recognize Patterns

325
00:14:43,280 --> 00:14:46,280
所以他可以將很多不同的講章

326
00:14:46,280 --> 00:14:47,280
Recognize Patterns

327
00:14:47,280 --> 00:14:48,280
砌一些東西出來

328
00:14:48,280 --> 00:14:50,280
但是他做不到的東西

329
00:14:50,280 --> 00:14:52,280
就是一些任何新的事物

330
00:14:52,280 --> 00:14:54,280
發生過的東西

331
00:14:54,280 --> 00:14:56,280
他是不能夠對應的

332
00:14:56,280 --> 00:15:00,280
所以一個講章如果是好的話

333
00:15:00,280 --> 00:15:04,280
應該是對應會眾的生活要面對的問題

334
00:15:04,280 --> 00:15:05,280
如果這樣的話

335
00:15:05,280 --> 00:15:08,280
其實GPT應該永遠都不能做到

336
00:15:08,280 --> 00:15:11,280
OK

337
00:15:11,280 --> 00:15:13,280
好,沒有人出來

338
00:15:13,280 --> 00:15:17,280
繼續問一些網上問的問題

339
00:15:17,280 --> 00:15:20,280
其實剛才提過一點

340
00:15:20,280 --> 00:15:21,280
好像是Felix

341
00:15:21,280 --> 00:15:22,280
不緊要是誰都好

342
00:15:22,280 --> 00:15:25,280
就是那個系統

343
00:15:25,280 --> 00:15:26,280
你提過系統

344
00:15:26,280 --> 00:15:27,280
不過好像Alex也提過

345
00:15:27,280 --> 00:15:30,280
就是技術都會形成一個人

346
00:15:30,280 --> 00:15:31,280
其實院長也有提過

347
00:15:31,280 --> 00:15:32,280
那個手取的問題

348
00:15:32,280 --> 00:15:35,280
即是手起枕那個

349
00:15:35,280 --> 00:15:38,280
其實你想像當譬如

350
00:15:38,280 --> 00:15:40,280
Check GPD或是越來越多AI

351
00:15:40,280 --> 00:15:42,280
或者好就是那些數學

352
00:15:42,280 --> 00:15:44,280
或者Check GPD不知道是好還是不好

353
00:15:44,280 --> 00:15:46,280
或者甚至比較malificious

354
00:15:46,280 --> 00:15:48,280
譬如好像騙你

355
00:15:48,280 --> 00:15:50,280
用AI走來騙你

356
00:15:50,280 --> 00:15:54,280
即是AI在我們的生活越來越多的時候

357
00:15:54,280 --> 00:15:58,280
其實對人的mentality會怎樣影響呢

358
00:15:58,280 --> 00:16:01,280
即是不是那個很big的story

359
00:16:01,280 --> 00:16:03,280
是比較concrete一點

360
00:16:03,280 --> 00:16:05,280
即是我經常對著手機

361
00:16:05,280 --> 00:16:06,280
或者經常和手機聊天

362
00:16:06,280 --> 00:16:07,280
和Check GPD聊天

363
00:16:07,280 --> 00:16:10,280
其實對人的mentality

364
00:16:10,280 --> 00:16:13,280
或者我們的心靈其實會有什麼影響呢

365
00:16:13,280 --> 00:16:15,280
我搶答先吧

366
00:16:15,280 --> 00:16:17,280
我不知道大家有沒有

367
00:16:17,280 --> 00:16:20,280
這個也是自我反省的一個過程

368
00:16:20,280 --> 00:16:26,280
近幾年或者自從手機用得多之後

369
00:16:26,280 --> 00:16:29,280
不知道大家對於耐性這件事

370
00:16:29,280 --> 00:16:31,280
是不是會弱了

371
00:16:31,280 --> 00:16:35,280
即是很多時候科技是告訴我們

372
00:16:35,280 --> 00:16:38,280
我要的東西我要的答案

373
00:16:38,280 --> 00:16:39,280
是要很即時

374
00:16:39,280 --> 00:16:42,280
即是以前我們想小時候的時候

375
00:16:42,280 --> 00:16:43,280
未必是

376
00:16:43,280 --> 00:16:46,280
即是現在我甚至

377
00:16:46,280 --> 00:16:48,280
剛才說開教育

378
00:16:48,280 --> 00:16:49,280
計算機

379
00:16:49,280 --> 00:16:53,280
我們以前真的要找人手計算

380
00:16:53,280 --> 00:16:54,280
現在也要的

381
00:16:54,280 --> 00:16:56,280
某些小學生也是人手計算

382
00:16:56,280 --> 00:16:57,280
去計算的時候

383
00:16:57,280 --> 00:17:01,280
其實是一個操練耐性的過程

384
00:17:01,280 --> 00:17:02,280
即是那種技巧

385
00:17:02,280 --> 00:17:07,280
但其實是如果說mentality的改變

386
00:17:07,280 --> 00:17:09,280
其實我不知道大家對於

387
00:17:09,280 --> 00:17:13,280
人與人之間的耐性

388
00:17:13,280 --> 00:17:15,280
即是我發了一個message

389
00:17:15,280 --> 00:17:17,280
是要秒回的

390
00:17:17,280 --> 00:17:20,280
年輕人的術語叫做秒回

391
00:17:20,280 --> 00:17:22,280
如果不是秒回

392
00:17:22,280 --> 00:17:26,280
是不是在著緊我

393
00:17:26,280 --> 00:17:27,280
或者發生什麼事

394
00:17:27,280 --> 00:17:32,280
這些是影響了我們與人之間的交流

395
00:17:32,280 --> 00:17:34,280
我們的耐性是弱了

396
00:17:34,280 --> 00:17:35,280
如果我說第一件事

397
00:17:35,280 --> 00:17:41,280
就是對於那種耐性失去的東西是弱了

398
00:17:41,280 --> 00:17:43,280
我就繼續回答

399
00:17:43,280 --> 00:17:47,280
其實當我們與教育委員會談話

400
00:17:47,280 --> 00:17:48,280
他很禮貌的

401
00:17:48,280 --> 00:17:50,280
你怎樣說他都會

402
00:17:50,280 --> 00:17:51,280
他做錯事的時候

403
00:17:51,280 --> 00:17:53,280
他會說I apologize第一句

404
00:17:53,280 --> 00:17:54,280
很舒服

405
00:17:54,280 --> 00:17:56,280
無論你怎樣罵他

406
00:17:56,280 --> 00:17:57,280
他都是這樣的

407
00:17:57,280 --> 00:18:00,280
很舒服的

408
00:18:00,280 --> 00:18:01,280
很吸引的

409
00:18:01,280 --> 00:18:04,280
你明白了

410
00:18:04,280 --> 00:18:07,280
其實我們對人不是這樣的

411
00:18:07,280 --> 00:18:11,280
人有一種叫做自由

412
00:18:11,280 --> 00:18:14,280
自由簡單來說就是你猜不到他

413
00:18:14,280 --> 00:18:17,280
猜不到他往往他的回應

414
00:18:17,280 --> 00:18:20,280
會令你不開心和不舒服

415
00:18:20,280 --> 00:18:22,280
因為人是這樣的

416
00:18:22,280 --> 00:18:27,280
其實科技是能夠讓我們舒舒服服

417
00:18:27,280 --> 00:18:30,280
所以就算check gdp的回應

418
00:18:30,280 --> 00:18:32,280
除非你寫一個program出來

419
00:18:32,280 --> 00:18:34,280
就沒有人喜歡

420
00:18:34,280 --> 00:18:36,280
因為科技是令你舒服

421
00:18:36,280 --> 00:18:44,280
它不會產生一些令你不舒服的東西

422
00:18:44,280 --> 00:18:46,280
但其實人與人的交往

423
00:18:46,280 --> 00:18:48,280
人與人的交往是這樣的

424
00:18:48,280 --> 00:18:53,280
甚至有些科技的哲學家幻想

425
00:18:53,280 --> 00:18:55,280
可能到某一個程度

426
00:18:55,280 --> 00:18:58,280
譬如有virtual reality

427
00:18:58,280 --> 00:19:01,280
一個人可能已經發展到一個地步

428
00:19:01,280 --> 00:19:06,280
我由頭到尾不需要和一些令我不開心的人接觸

429
00:19:06,280 --> 00:19:11,280
所有人都只是被一些很舒服的科技AI圍著

430
00:19:11,280 --> 00:19:13,280
問的問題好不好呢?

431
00:19:13,280 --> 00:19:15,280
可能人會覺得很好

432
00:19:15,280 --> 00:19:18,280
我們值得問,是否真的想這樣的世界?

433
00:19:18,280 --> 00:19:19,280
甚麼叫做人?

434
00:19:19,280 --> 00:19:22,280
我們剛才說了操練美德

435
00:19:22,280 --> 00:19:24,280
操練美德何時發生?

436
00:19:24,280 --> 00:19:26,280
是當你見到一些很難受的操練到

437
00:19:26,280 --> 00:19:28,280
你對著一些很舒服的人

438
00:19:28,280 --> 00:19:29,280
你沒有美德可操練

439
00:19:29,280 --> 00:19:30,280
甚麼叫彼此相愛?

440
00:19:30,280 --> 00:19:32,280
因為那些不可愛的人在你面前

441
00:19:32,280 --> 00:19:34,280
你可愛的人每個人都喜歡

442
00:19:34,280 --> 00:19:36,280
所以很有趣

443
00:19:36,280 --> 00:19:38,280
科技究竟是如何塑造我們呢?

444
00:19:38,280 --> 00:19:44,280
一些這麼方便、開心的東西究竟是好還是不好呢?

445
00:19:44,280 --> 00:19:47,280
就問我們做人的目標是甚麼

446
00:19:47,280 --> 00:19:51,280
其實是一個終末觀

447
00:19:51,280 --> 00:19:55,280
即是我的目標,目的想做一個甚麼人

448
00:19:55,280 --> 00:19:57,280
其實是要回答這個問題

449
00:19:58,280 --> 00:20:04,280
我想說一個問題令我想起

450
00:20:04,280 --> 00:20:09,280
我在想高科技和科技是有分別的

451
00:20:09,280 --> 00:20:13,280
高科技變得快到你適應不到

452
00:20:13,280 --> 00:20:18,280
如果你想像我們所有科技都攤長二億年慢慢轉變

453
00:20:18,280 --> 00:20:21,280
那些人可能容易很多去適應

454
00:20:21,280 --> 00:20:24,280
但因為在你未適應到一個新的科技之前

455
00:20:24,280 --> 00:20:28,280
譬如說社交媒體突然間又說AI

456
00:20:28,280 --> 00:20:30,280
或者一百年前都沒有飛機

457
00:20:30,280 --> 00:20:31,280
對於我祖母來說

458
00:20:31,280 --> 00:20:35,280
你給她電腦她沒可能知道發生甚麼事

459
00:20:35,280 --> 00:20:37,280
可能到了五十年之後

460
00:20:37,280 --> 00:20:39,280
我死了五十年之後

461
00:20:39,280 --> 00:20:41,280
三十年之後到我老了的時候

462
00:20:41,280 --> 00:20:44,280
可能已經有很多東西我適應不到

463
00:20:44,280 --> 00:20:47,280
而我想其中一個問題要面對

464
00:20:47,280 --> 00:20:49,280
為甚麼這麼大問題呢?

465
00:20:49,280 --> 00:20:51,280
就是因為那些東西來得太快

466
00:20:51,280 --> 00:20:55,280
整個社會都沒有空間去適應

467
00:20:55,280 --> 00:20:57,280
引起很多問題

468
00:20:57,280 --> 00:20:59,280
其實也有些問題

469
00:20:59,280 --> 00:21:04,280
Alex剛才很強調監管

470
00:21:04,280 --> 00:21:07,280
問題是現在的科技走得這麼快

471
00:21:07,280 --> 00:21:11,280
固然像我這些凡夫俗子都未搞定

472
00:21:11,280 --> 00:21:13,280
怎樣去監管呢?

473
00:21:13,280 --> 00:21:15,280
有人說Twitter

474
00:21:15,280 --> 00:21:17,280
現在好像Twitter改了名叫X

475
00:21:17,280 --> 00:21:19,280
以前經常說Twitter

476
00:21:19,280 --> 00:21:21,280
傳來傳去

477
00:21:21,280 --> 00:21:22,280
我都不搞了

478
00:21:22,280 --> 00:21:24,280
我只是搞Facebook一個已經頭都大了

479
00:21:24,280 --> 00:21:28,280
所以其他Twitter、Line那些我不管了

480
00:21:28,280 --> 00:21:32,280
你不懂你怎樣去監管呢?

481
00:21:32,280 --> 00:21:34,280
這個是比較個人的水平

482
00:21:34,280 --> 00:21:36,280
就算是社會水平來說

483
00:21:36,280 --> 00:21:37,280
監管

484
00:21:37,280 --> 00:21:42,280
其實我覺得奧巴馬電影是頗啟迪的

485
00:21:42,280 --> 00:21:44,280
奧巴馬發展的原子彈

486
00:21:44,280 --> 00:21:47,280
最後是否扔掉是Truman決定的

487
00:21:47,280 --> 00:21:50,280
所以很多時候最後所謂的監管權

488
00:21:50,280 --> 00:21:53,280
其實在社會裏面是depend on

489
00:21:53,280 --> 00:21:55,280
那些politician

490
00:21:55,280 --> 00:21:58,280
和depend on那些大商家

491
00:21:58,280 --> 00:22:00,280
Aaron Marx那些人

492
00:22:00,280 --> 00:22:01,280
經常說那些

493
00:22:01,280 --> 00:22:03,280
我覺得Aaron Marx經常說那些瘋狂的東西

494
00:22:03,280 --> 00:22:05,280
你想想

495
00:22:05,280 --> 00:22:07,280
如果你經常說監管

496
00:22:07,280 --> 00:22:09,280
我們要將監管權交在

497
00:22:09,280 --> 00:22:13,280
這批政客和這批大商家當中

498
00:22:13,280 --> 00:22:15,280
到底是不是realistic呢?

499
00:22:15,280 --> 00:22:17,280
到底怎樣監管呢?

500
00:22:17,280 --> 00:22:21,280
我想補充一件事

501
00:22:21,280 --> 00:22:23,280
因為我那張slide就有說

502
00:22:23,280 --> 00:22:25,280
Elon Musk有petition

503
00:22:25,280 --> 00:22:27,280
去暫停那個GDP的development

504
00:22:27,280 --> 00:22:29,280
而大部分人認為

505
00:22:29,280 --> 00:22:31,280
因為他想自己develop自己那套東西

506
00:22:31,280 --> 00:22:35,280
所以就要求他暫停

507
00:22:35,280 --> 00:22:39,280
我想我剛才是很強調

508
00:22:39,280 --> 00:22:41,280
正正是因為我們沒有辦法

509
00:22:41,280 --> 00:22:45,280
去在權力的角力裏面

510
00:22:45,280 --> 00:22:47,280
去有那種話事權

511
00:22:47,280 --> 00:22:52,280
所以反而我們能夠做的事

512
00:22:52,280 --> 00:22:54,280
首先是我們自己

513
00:22:54,280 --> 00:22:56,280
作為基督徒教會

514
00:22:56,280 --> 00:22:58,280
就是怎樣去demonstrate一種

515
00:22:58,280 --> 00:23:02,280
有德性、有道德的使用者

516
00:23:02,280 --> 00:23:04,280
或者是一個開發者

517
00:23:04,280 --> 00:23:05,280
是甚麼來的呢?

518
00:23:05,280 --> 00:23:08,280
讓這個世界其實可以見到

519
00:23:08,280 --> 00:23:10,280
原來是有一種不同的可能性

520
00:23:10,280 --> 00:23:13,280
然後這些影響力會不會慢慢地

521
00:23:13,280 --> 00:23:18,280
滲透到我們一些政客的手上呢?

522
00:23:18,280 --> 00:23:19,280
我不知道

523
00:23:19,280 --> 00:23:21,280
不過我想上帝給我們的

524
00:23:21,280 --> 00:23:25,280
就是有能力在被影響的空間

525
00:23:25,280 --> 00:23:26,280
首先我們要做好自己

526
00:23:26,280 --> 00:23:29,280
這是我自己的看法

527
00:23:29,280 --> 00:23:32,280
我想說兩個難題

528
00:23:32,280 --> 00:23:34,280
對於剛才說監管的東西

529
00:23:34,280 --> 00:23:35,280
第一個就是

530
00:23:35,280 --> 00:23:37,280
就好像那齣戲裡面

531
00:23:37,280 --> 00:23:39,280
Oppenheimer的戲裡面

532
00:23:39,280 --> 00:23:41,280
因為不同的國家

533
00:23:41,280 --> 00:23:43,280
或者不同的公司都在競爭

534
00:23:43,280 --> 00:23:44,280
有些人就會說

535
00:23:44,280 --> 00:23:47,280
如果我們這個國家說暫停的

536
00:23:47,280 --> 00:23:48,280
別人說是發展的

537
00:23:48,280 --> 00:23:50,280
那我們就全輸了

538
00:23:50,280 --> 00:23:53,280
現在說的不是剛才說的GPT那些東西

539
00:23:53,280 --> 00:23:55,280
很多軍事的武器

540
00:23:55,280 --> 00:23:57,280
其實都是用AI來做的

541
00:23:57,280 --> 00:23:59,280
有個朋友就跟我說

542
00:23:59,280 --> 00:24:01,280
Ukraine用的武器

543
00:24:01,280 --> 00:24:05,280
其實很多都是用Drones或者AI來做的

544
00:24:05,280 --> 00:24:07,280
這是其中一個問題

545
00:24:07,280 --> 00:24:08,280
第二個問題就是

546
00:24:08,280 --> 00:24:11,280
現在用的那套Deep Learning的做法

547
00:24:11,280 --> 00:24:13,280
其實是沒有人可以知道裡面發生什麼事的

548
00:24:13,280 --> 00:24:17,280
就是by default一定是沒有可能知道裡面發生什麼事的

549
00:24:17,280 --> 00:24:21,280
所以如果要解釋他為什麼達到那個決定

550
00:24:21,280 --> 00:24:22,280
其實是

551
00:24:22,280 --> 00:24:24,280
你不如叫他不要做算了

552
00:24:24,280 --> 00:24:30,280
譬如你說為什麼GPT可以分析到我們說話的呢?

553
00:24:30,280 --> 00:24:32,280
其實沒有人真的知道

554
00:24:32,280 --> 00:24:34,280
你最多可以做到猜猜的

555
00:24:34,280 --> 00:24:35,280
但是做不到

556
00:24:35,280 --> 00:24:36,280
如果做不到的時候

557
00:24:36,280 --> 00:24:40,280
剛才說的那種transparency的要求

558
00:24:40,280 --> 00:24:42,280
其實是很難做到的

559
00:24:42,280 --> 00:24:48,280
這個對於現在做研究的人來說是一個謎

560
00:24:48,280 --> 00:24:50,280
補充一點

561
00:24:50,280 --> 00:24:53,280
我以前也聽過所謂的Deep Learning

562
00:24:53,280 --> 00:24:54,280
其實很簡單的說法就是

563
00:24:54,280 --> 00:24:56,280
你叫電腦自己和自己下棋

564
00:24:56,280 --> 00:24:59,280
電腦下了幾天

565
00:24:59,280 --> 00:25:01,280
可能下了一萬盤棋

566
00:25:01,280 --> 00:25:02,280
他就學會下棋了

567
00:25:02,280 --> 00:25:05,280
他就比全世界下棋最厲害的人厲害了

568
00:25:05,280 --> 00:25:07,280
但是你不知道電腦怎麼想的

569
00:25:07,280 --> 00:25:09,280
他只是自己和自己下了一輪

570
00:25:09,280 --> 00:25:11,280
他就學會下棋了

571
00:25:11,280 --> 00:25:13,280
這個就是Deep Learning

572
00:25:13,280 --> 00:25:16,280
所以我們根本不知道裡面發生什麼事

573
00:25:16,280 --> 00:25:19,280
所以聽完你說也挺悲觀的

574
00:25:19,280 --> 00:25:22,280
有些研究是用解釋AI的

575
00:25:22,280 --> 00:25:25,280
嘗試說我想讓他真的解釋得到

576
00:25:25,280 --> 00:25:27,280
只不過我自己覺得他做不到

577
00:25:27,280 --> 00:25:29,280
可能有些人真的做到

578
00:25:29,280 --> 00:25:31,280
但是我覺得他做到的可能性就是

579
00:25:31,280 --> 00:25:34,280
在中間加多幾個步驟

580
00:25:34,280 --> 00:25:36,280
而步驟是可以解釋的

581
00:25:36,280 --> 00:25:38,280
不過步驟裡面的東西仍然解釋不了

582
00:25:38,280 --> 00:25:42,280
有點頭痛

583
00:25:42,280 --> 00:25:43,280
Anyway

584
00:25:43,280 --> 00:25:47,280
有人問了一個挺有趣的問題

585
00:25:47,280 --> 00:25:49,280
他說你剛才說的AI

586
00:25:49,280 --> 00:25:55,280
都是集中對香港、美國這些所謂現代社會的影響

587
00:25:55,280 --> 00:25:58,280
他想問特別對貧窮世界

588
00:25:58,280 --> 00:26:02,280
其實這些AI或者最尖端的科技

589
00:26:02,280 --> 00:26:04,280
會有什麼影響呢?

590
00:26:04,280 --> 00:26:06,280
你猜是好的多壞的多?

591
00:26:06,280 --> 00:26:21,280
其實有一個現象也不只是AI

592
00:26:21,280 --> 00:26:23,280
普遍的科技

593
00:26:23,280 --> 00:26:25,280
有一個叫Digital Divide

594
00:26:25,280 --> 00:26:29,280
有些人就能夠掌控到知道

595
00:26:29,280 --> 00:26:32,280
因為現在的世界

596
00:26:32,280 --> 00:26:37,280
這些科技、AI當然是用來更加有能力去掌控世界

597
00:26:37,280 --> 00:26:41,280
所以一定是一種權力上的差異

598
00:26:41,280 --> 00:26:48,280
永遠都是發展了的那些所謂著數

599
00:26:48,280 --> 00:26:50,280
然後就去敲或者挪奕

600
00:26:50,280 --> 00:26:56,280
那個會是一個不好、一個不公平的現象

601
00:26:56,280 --> 00:27:01,280
我想那個看法都離不開

602
00:27:01,280 --> 00:27:07,280
如何能夠在制度上或做法上幫助到

603
00:27:07,280 --> 00:27:14,280
令到更多人能夠接觸到、運用到、有益到

604
00:27:14,280 --> 00:27:16,280
這是一個很初步的答案

605
00:27:16,280 --> 00:27:18,280
你會否有不同的看法呢?

606
00:27:18,280 --> 00:27:19,280
你說起Digital Divide

607
00:27:19,280 --> 00:27:23,280
我就有一個比較吊詭的回應

608
00:27:23,280 --> 00:27:26,280
正正因為他們不在網上

609
00:27:26,280 --> 00:27:28,280
就不會收集到他們的資料

610
00:27:28,280 --> 00:27:29,280
他們反而會好一點

611
00:27:29,280 --> 00:27:31,280
起碼不會被人操縱

612
00:27:31,280 --> 00:27:34,280
他們仍然可以很開心地在大自然生活

613
00:27:34,280 --> 00:27:37,280
我們就被人操控自己都不知道

614
00:27:37,280 --> 00:27:41,280
所以問題是甚麼叫做好一點呢?

615
00:27:41,280 --> 00:27:43,280
有趣的地方就是

616
00:27:43,280 --> 00:27:46,280
剛才你說不應該看那些feed

617
00:27:46,280 --> 00:27:47,280
給你feed就去

618
00:27:47,280 --> 00:27:50,280
然後你的購物的pattern又被人知道了

619
00:27:50,280 --> 00:27:51,280
可能反過來

620
00:27:51,280 --> 00:27:53,280
他正正就是給我要的東西

621
00:27:53,280 --> 00:27:55,280
我很喜歡被人操控

622
00:27:55,280 --> 00:27:56,280
很有趣的

623
00:27:56,280 --> 00:27:58,280
究竟甚麼叫做好一點呢?

624
00:27:58,280 --> 00:28:04,280
可能有人會覺得被操控、被監控是很好的

625
00:28:04,280 --> 00:28:07,280
所以《美麗新世界》的故事就是

626
00:28:07,280 --> 00:28:11,280
一班很喜歡生活的人

627
00:28:11,280 --> 00:28:14,280
我又落地多一點

628
00:28:14,280 --> 00:28:17,280
這個問題我又想起

629
00:28:17,280 --> 00:28:19,280
幾年前有個畫面

630
00:28:19,280 --> 00:28:21,280
大家記得疫情的時候

631
00:28:21,280 --> 00:28:25,280
有些地方要拿著手機才能進去

632
00:28:25,280 --> 00:28:27,280
甚至超級市場、街市

633
00:28:27,280 --> 00:28:28,280
大家有印象

634
00:28:28,280 --> 00:28:31,280
我經常很深刻的畫面就是

635
00:28:31,280 --> 00:28:34,280
有些婆婆或者老人家

636
00:28:34,280 --> 00:28:36,280
不用去到非洲那麼遠

637
00:28:36,280 --> 00:28:39,280
你只需要一些在科技上

638
00:28:39,280 --> 00:28:42,280
能力相對比較弱的人

639
00:28:42,280 --> 00:28:46,280
我們在一個科技社會裏

640
00:28:46,280 --> 00:28:49,280
他們就會成為那種很弱勢的人

641
00:28:49,280 --> 00:28:51,280
所以剛才也有提及

642
00:28:51,280 --> 00:28:56,280
我們作為使用者

643
00:28:56,280 --> 00:29:00,280
我們如何確保這班弱勢

644
00:29:00,280 --> 00:29:02,280
需要照顧的人

645
00:29:02,280 --> 00:29:05,280
得到他們生活上應有的尊嚴

646
00:29:05,280 --> 00:29:07,280
這個是要去想的問題

647
00:29:07,280 --> 00:29:08,280
我補充一句

648
00:29:08,280 --> 00:29:10,280
當然剛才Alex說的

649
00:29:10,280 --> 00:29:11,280
不過我們又覺得

650
00:29:11,280 --> 00:29:13,280
因為科技發展到一個地步

651
00:29:13,280 --> 00:29:15,280
我們有很多假設

652
00:29:15,280 --> 00:29:18,280
如果中辰你要來報讀

653
00:29:18,280 --> 00:29:21,280
現在是不可能沒有電腦的

654
00:29:21,280 --> 00:29:25,280
但我們在沒有任何一年

655
00:29:25,280 --> 00:29:26,280
突然在申請表上說

656
00:29:26,280 --> 00:29:28,280
你一定要有電腦才能來讀

657
00:29:28,280 --> 00:29:29,280
是不是?

658
00:29:29,280 --> 00:29:30,280
很有趣的

659
00:29:30,280 --> 00:29:32,280
我們做了很多假設

660
00:29:32,280 --> 00:29:33,280
可能我讀的年代

661
00:29:33,280 --> 00:29:35,280
真的沒有電腦也可以的

662
00:29:35,280 --> 00:29:37,280
可以原稿子交功課

663
00:29:37,280 --> 00:29:39,280
但今天是完全不行的

664
00:29:39,280 --> 00:29:42,280
我們中間沒有做過任何一個動作

665
00:29:42,280 --> 00:29:45,280
是要要求你有電腦

666
00:29:45,280 --> 00:29:47,280
或是懂電腦才能讀書

667
00:29:47,280 --> 00:29:49,280
很有趣的就是

668
00:29:49,280 --> 00:29:53,280
有沒有科技的問題

669
00:29:53,280 --> 00:29:55,280
仿佛我們是有部份

670
00:29:55,280 --> 00:29:56,280
而我們現在有的人

671
00:29:56,280 --> 00:29:58,280
是完全不會想像到

672
00:29:58,280 --> 00:30:00,280
沒有是一個怎樣的世界

673
00:30:00,280 --> 00:30:04,280
所以現在你沒有智能電話的話

674
00:30:04,280 --> 00:30:07,280
你去到餐廳也可能點不到菜

675
00:30:07,280 --> 00:30:09,280
掃描不到QR Code

676
00:30:09,280 --> 00:30:15,280
所以要繼續想這個問題

677
00:30:15,280 --> 00:30:20,280
或者轉一轉方向

678
00:30:20,280 --> 00:30:21,280
再次鼓勵

679
00:30:21,280 --> 00:30:25,280
有人想站出來做勇敢的AI人

680
00:30:25,280 --> 00:30:28,280
站出來問問題是很歡迎的

681
00:30:28,280 --> 00:30:31,280
不過未有人站出來再問一個問題

682
00:30:31,280 --> 00:30:32,280
也是有人問的

683
00:30:32,280 --> 00:30:36,280
方向有一點點有關

684
00:30:36,280 --> 00:30:37,280
變了一點點

685
00:30:37,280 --> 00:30:41,280
作為一個堂會的教授

686
00:30:41,280 --> 00:30:45,280
其實你覺得堂會教授

687
00:30:45,280 --> 00:30:51,280
有什麼責任幫助會友去面對AI世代呢?

688
00:30:51,280 --> 00:30:53,280
無論是年長的

689
00:30:53,280 --> 00:30:55,280
死死也

690
00:30:55,280 --> 00:30:58,280
有沒有責任幫助

691
00:30:58,280 --> 00:30:59,280
或者年輕的

692
00:30:59,280 --> 00:31:02,280
他們將來面對很多AI的東西

693
00:31:02,280 --> 00:31:03,280
會爆出來

694
00:31:03,280 --> 00:31:04,280
想都想不到

695
00:31:04,280 --> 00:31:10,280
堂會所謂牧養或者門徒訓練的時候

696
00:31:10,280 --> 00:31:14,280
有沒有什麼可以做或者應該做的呢?

697
00:31:14,280 --> 00:31:18,280
誰都可以答

698
00:31:18,280 --> 00:31:20,280
我們三個都不是堂會負責人

699
00:31:20,280 --> 00:31:24,280
其實這個問題

700
00:31:24,280 --> 00:31:26,280
我覺得問什麼是教牧

701
00:31:26,280 --> 00:31:28,280
多於問AI

702
00:31:28,280 --> 00:31:31,280
其實你說教牧是在講牧養人

703
00:31:31,280 --> 00:31:34,280
關顧他整個人的生命

704
00:31:34,280 --> 00:31:36,280
而AI是這麼重要的一部分

705
00:31:36,280 --> 00:31:38,280
當然是責無旁貸

706
00:31:38,280 --> 00:31:41,280
不過世界上真的有這麼多東西

707
00:31:41,280 --> 00:31:44,280
其實我想問題是什麼呢?

708
00:31:44,280 --> 00:31:46,280
就是要留意當你的會眾

709
00:31:46,280 --> 00:31:51,280
你給某一樣東西影響得深遠

710
00:31:51,280 --> 00:31:53,280
那個就成為你牧養的議題

711
00:31:53,280 --> 00:31:55,280
我會覺得是這樣

712
00:31:55,280 --> 00:31:57,280
當然當你去到這個位置的時候

713
00:31:57,280 --> 00:31:59,280
你的困難的地方就在於

714
00:31:59,280 --> 00:32:02,280
這個教牧需要知道到什麼深度

715
00:32:02,280 --> 00:32:04,280
究竟影響怎樣

716
00:32:04,280 --> 00:32:09,280
實質上是有些困難的

717
00:32:09,280 --> 00:32:11,280
我想我會這樣認為

718
00:32:11,280 --> 00:32:17,280
我就從會友或者是評論的角度

719
00:32:17,280 --> 00:32:19,280
去期望一下

720
00:32:19,280 --> 00:32:20,280
教牧

721
00:32:20,280 --> 00:32:23,280
如果作為一個職場的人

722
00:32:23,280 --> 00:32:25,280
我回到教會

723
00:32:25,280 --> 00:32:31,280
我是被在我的科技或者工作裏面

724
00:32:31,280 --> 00:32:33,280
有很多的影響

725
00:32:33,280 --> 00:32:37,280
或者是我很渴望我的牧者能夠

726
00:32:37,280 --> 00:32:42,280
我不需要他知道所有科技的東西

727
00:32:42,280 --> 00:32:46,280
但是他會肯聽我說一下我的掙扎

728
00:32:46,280 --> 00:32:48,280
我面對的問題

729
00:32:48,280 --> 00:32:51,280
倫理上的一些挑戰等等

730
00:32:51,280 --> 00:32:55,280
關於科技的事情

731
00:32:55,280 --> 00:32:57,280
或者是我要去訂立一些

732
00:32:57,280 --> 00:33:00,280
我的工作是要去做一些政策

733
00:33:00,280 --> 00:33:02,280
是會影響到很多人的

734
00:33:02,280 --> 00:33:04,280
教牧在這個角色裏面

735
00:33:04,280 --> 00:33:07,280
其實是很需要同行同聆聽

736
00:33:07,280 --> 00:33:10,280
他未必會給到很多很具體的

737
00:33:10,280 --> 00:33:13,280
或者是一些很直接的東西

738
00:33:13,280 --> 00:33:16,280
但是他要明白和理解我的困難

739
00:33:16,280 --> 00:33:21,280
這個是我想會得到的一個回應

740
00:33:21,280 --> 00:33:27,280
我不是教牧

741
00:33:27,280 --> 00:33:31,280
不過我認識一個朋友是傳道人

742
00:33:31,280 --> 00:33:34,280
他跟我說最近的年輕人很灰心

743
00:33:34,280 --> 00:33:37,280
因為覺得將來AI都做了所有的東西

744
00:33:37,280 --> 00:33:39,280
我們可以做些什麼呢?

745
00:33:39,280 --> 00:33:41,280
如果面對一些

746
00:33:41,280 --> 00:33:44,280
他們有這樣的心態的時候

747
00:33:44,280 --> 00:33:47,280
當然牧者是需要想方法去回應

748
00:33:47,280 --> 00:33:58,280
好,又問了一個比較天馬行空的問題

749
00:33:58,280 --> 00:33:59,280
其實

750
00:33:59,280 --> 00:34:02,280
我回答你的問題

751
00:34:02,280 --> 00:34:07,280
三位有一天AI會不會可能被注入感情呢?

752
00:34:07,280 --> 00:34:09,280
縱然感情對AI來說

753
00:34:09,280 --> 00:34:11,280
起初是一些數據

754
00:34:11,280 --> 00:34:16,280
如果AI真的有了感情創意

755
00:34:16,280 --> 00:34:20,280
會不會叫它一個人呢?

756
00:34:20,280 --> 00:34:23,280
它有沒有靈魂呢?

757
00:34:23,280 --> 00:34:26,280
你怎樣看這個問題呢?

758
00:34:26,280 --> 00:34:30,280
如果你問

759
00:34:30,280 --> 00:34:33,280
現在我剛才所說的

760
00:34:33,280 --> 00:34:38,280
所有最近這十年來的突破性發展就是Deep Learning

761
00:34:38,280 --> 00:34:40,280
Deep Learning做到的就是Pattern Recognition

762
00:34:40,280 --> 00:34:43,280
所以如果只是靠這個突破性的發展

763
00:34:43,280 --> 00:34:46,280
我就覺得不可能有感情

764
00:34:46,280 --> 00:34:48,280
我不敢答說將來還有什麼突破

765
00:34:48,280 --> 00:34:49,280
會不會令到有感情

766
00:34:49,280 --> 00:34:54,280
但起碼在有生之年應該都不會見到有感情的AI

767
00:34:54,280 --> 00:35:01,280
我從一個基礎的角度去討論

768
00:35:01,280 --> 00:35:05,280
人的感情其實是一些

769
00:35:05,280 --> 00:35:09,280
因為我們人的腦袋有身體反應

770
00:35:09,280 --> 00:35:14,280
一些細胞裏面的化學物

771
00:35:14,280 --> 00:35:17,280
如果從學理的角度來說

772
00:35:17,280 --> 00:35:21,280
現在AI全部在電子平台下

773
00:35:21,280 --> 00:35:23,280
一同在運作

774
00:35:23,280 --> 00:35:25,280
所以

775
00:35:25,280 --> 00:35:28,280
什麼叫感情呢?

776
00:35:28,280 --> 00:35:31,280
如果我們人有一個感覺

777
00:35:31,280 --> 00:35:35,280
那種感覺其實跟AI在電腦平台是完全不同的

778
00:35:35,280 --> 00:35:37,280
如果從這樣的角度來說

779
00:35:37,280 --> 00:35:39,280
就一定沒有感情

780
00:35:39,280 --> 00:35:44,280
不過如果你說只是看表情

781
00:35:44,280 --> 00:35:48,280
機械人對答的語氣和內容

782
00:35:48,280 --> 00:35:51,280
而你將那些東西看為感情的全部

783
00:35:51,280 --> 00:35:53,280
那他可以有感情

784
00:35:53,280 --> 00:35:55,280
我剛才也有說

785
00:35:55,280 --> 00:36:00,280
其實科技是一個表面現象的重視

786
00:36:00,280 --> 00:36:04,280
所以現在都會有一些

787
00:36:04,280 --> 00:36:06,280
譬如AI輔導員

788
00:36:06,280 --> 00:36:09,280
不是說什麼

789
00:36:09,280 --> 00:36:11,280
因為有研究說

790
00:36:11,280 --> 00:36:17,280
有人對一些他知道是AI的輔導員更加坦誠地說話

791
00:36:17,280 --> 00:36:20,280
大家明白為什麼嗎?

792
00:36:20,280 --> 00:36:25,280
所以是不是要否定他能夠成為這些人的談心幫助

793
00:36:25,280 --> 00:36:27,280
這是另一個問題

794
00:36:27,280 --> 00:36:29,280
我不敢回答是或不是

795
00:36:29,280 --> 00:36:31,280
你明白嗎?

796
00:36:31,280 --> 00:36:33,280
複雜的問題

797
00:36:33,280 --> 00:36:35,280
輔導員被取代的問題

798
00:36:35,280 --> 00:36:41,280
但是人其實很容易將對著的東西

799
00:36:41,280 --> 00:36:45,280
變成以為他是人去對答

800
00:36:45,280 --> 00:36:49,280
有個詞語叫做Attribution Policy

801
00:36:49,280 --> 00:36:53,280
我們很喜歡將周圍的東西

802
00:36:53,280 --> 00:36:57,280
譬如貓、狗都會想到像人一樣

803
00:36:57,280 --> 00:36:59,280
連氣都一樣

804
00:36:59,280 --> 00:37:01,280
你真的很有空

805
00:37:01,280 --> 00:37:06,280
因為女兒喜歡看

806
00:37:06,280 --> 00:37:14,280
剛才說有一個東西叫做Social Robot

807
00:37:14,280 --> 00:37:21,280
Social Robot會做到像人有的感情跟你互動

808
00:37:21,280 --> 00:37:27,280
有些資料說這種互動叫做Asymetric感情

809
00:37:27,280 --> 00:37:29,280
即是你跟他有感情

810
00:37:29,280 --> 00:37:31,280
他可以假裝感情出來

811
00:37:31,280 --> 00:37:33,280
但其實他沒有感情

812
00:37:33,280 --> 00:37:38,280
剛才說到AI可以做到任何的模式

813
00:37:38,280 --> 00:37:43,280
當然可以假裝成一個人去表達每一種感情

814
00:37:43,280 --> 00:37:46,280
可以複雜到一個地步

815
00:37:46,280 --> 00:37:49,280
你是分辨不到一個人和一個AI的感情

816
00:37:49,280 --> 00:37:51,280
背後他是不是真的有感情呢?

817
00:37:51,280 --> 00:37:54,280
這個可以說是一個更深的哲學問題

818
00:37:54,280 --> 00:37:56,280
我們也不會再說

819
00:37:56,280 --> 00:38:02,280
我想分開感情和感官感受

820
00:38:02,280 --> 00:38:04,280
如果你去發展AI

821
00:38:04,280 --> 00:38:06,280
讓他可以有觸感

822
00:38:06,280 --> 00:38:07,280
或者是味覺

823
00:38:07,280 --> 00:38:09,280
分辨到甜酸苦辣

824
00:38:09,280 --> 00:38:11,280
我覺得是有可能的

825
00:38:11,280 --> 00:38:15,280
但是當你吃完一件東西覺得它很好吃

826
00:38:15,280 --> 00:38:18,280
我講一個自己的經歷

827
00:38:18,280 --> 00:38:20,280
每天回到中晨

828
00:38:20,280 --> 00:38:24,280
有位同工會沖一杯咖啡給我喝

829
00:38:24,280 --> 00:38:27,280
那種感情、感受

830
00:38:27,280 --> 00:38:29,280
不單純是那杯咖啡

831
00:38:29,280 --> 00:38:36,280
而是覺得被支持、被幫助

832
00:38:36,280 --> 00:38:39,280
然後你品嘗那杯咖啡的時候

833
00:38:39,280 --> 00:38:41,280
有時候我們同工在聊天

834
00:38:41,280 --> 00:38:45,280
有茶味、有什麼果味

835
00:38:45,280 --> 00:38:49,280
這些其實是一種對那件事的欣賞

836
00:38:49,280 --> 00:38:51,280
對味道的一種欣賞

837
00:38:51,280 --> 00:38:57,280
我覺得AI未必會產生到那種對味道的欣賞

838
00:38:57,280 --> 00:38:59,280
對愛、對關係的那種東西

839
00:38:59,280 --> 00:39:01,280
其實不會有

840
00:39:01,280 --> 00:39:03,280
我想補充一件事

841
00:39:03,280 --> 00:39:07,280
初期有一個電腦科學家

842
00:39:07,280 --> 00:39:11,280
寫了一個程式叫Elisa

843
00:39:11,280 --> 00:39:13,280
Elisa是一個名字

844
00:39:13,280 --> 00:39:16,280
那個程式是可以與人互動的

845
00:39:16,280 --> 00:39:18,280
秘書也有參與

846
00:39:18,280 --> 00:39:21,280
所以秘書也知道背後全部都是硬碟

847
00:39:21,280 --> 00:39:24,280
沒有真正的知識或情緒

848
00:39:24,280 --> 00:39:27,280
然後秘書就和Elisa玩耍

849
00:39:27,280 --> 00:39:32,280
那個電腦科學家就在外面等

850
00:39:32,280 --> 00:39:34,280
上次進來的時候

851
00:39:34,280 --> 00:39:36,280
秘書就趕走了她

852
00:39:36,280 --> 00:39:38,280
就說你不要妨礙我與她聊天

853
00:39:38,280 --> 00:39:40,280
因為她已經情緒上

854
00:39:40,280 --> 00:39:43,280
與她明知是硬碟的程式有關

855
00:39:43,280 --> 00:39:45,280
所以這個例子

856
00:39:45,280 --> 00:39:49,280
剛才我說的attribution policy

857
00:39:49,280 --> 00:39:51,280
又可以叫做Elisa effect

858
00:39:51,280 --> 00:39:56,280
就是人真的很喜歡自己去投射一些感情

859
00:39:56,280 --> 00:39:58,280
到一些死物身上

860
00:39:58,280 --> 00:40:00,280
我不知道這些叫不叫做idolism

861
00:40:00,280 --> 00:40:04,280
但也看到人是多麼的脆弱

862
00:40:04,280 --> 00:40:06,280
而很多時候AI的發展

863
00:40:06,280 --> 00:40:09,280
都是嘗試針對如何去欺騙人的情感

864
00:40:09,280 --> 00:40:12,280
去寫出來

865
00:40:12,280 --> 00:40:15,280
你剛才說的例子

866
00:40:15,280 --> 00:40:19,280
有另一部電影叫做HER

867
00:40:19,280 --> 00:40:23,280
說一個人愛上了那個程式

868
00:40:23,280 --> 00:40:25,280
叫什麼名字呢?

869
00:40:25,280 --> 00:40:27,280
不過相信提到這件事

870
00:40:27,280 --> 00:40:29,280
剛才我們說responsibility

871
00:40:29,280 --> 00:40:31,280
其實都是集中說

872
00:40:31,280 --> 00:40:33,280
人怎樣用AI

873
00:40:33,280 --> 00:40:35,280
以致是bring good

874
00:40:35,280 --> 00:40:37,280
不是bring harm

875
00:40:37,280 --> 00:40:39,280
但是從另一個角度來看

876
00:40:39,280 --> 00:40:45,280
人對AI本身有沒有一些倫理的obligation呢?

877
00:40:45,280 --> 00:40:48,280
舉一個極端的例子

878
00:40:48,280 --> 00:40:50,280
Westworld那套電視劇

879
00:40:50,280 --> 00:40:53,280
製造了一個robot的世界出來

880
00:40:53,280 --> 00:40:56,280
讓人可以去打那些robot

881
00:40:56,280 --> 00:40:58,280
強姦那些女robot

882
00:40:58,280 --> 00:40:59,280
沒有事情發生

883
00:40:59,280 --> 00:41:01,280
因為她是robot

884
00:41:01,280 --> 00:41:05,280
譬如有一天製造了一個Westworld

885
00:41:05,280 --> 00:41:07,280
有沒有ethical implication

886
00:41:07,280 --> 00:41:10,280
強姦一個女robot

887
00:41:10,280 --> 00:41:12,280
或者looks like a woman的robot

888
00:41:12,280 --> 00:41:14,280
是不是一種先呢?

889
00:41:14,280 --> 00:41:17,280
有沒有什麼ethical implication呢?

890
00:41:17,280 --> 00:41:20,280
如果以一個Christian的角度來看

891
00:41:20,280 --> 00:41:22,280
你看色情片都是先

892
00:41:22,280 --> 00:41:25,280
所以這樣看一定是先

893
00:41:25,280 --> 00:41:29,280
至於究竟有沒有傷害到AI

894
00:41:29,280 --> 00:41:31,280
我都不懂回答

895
00:41:31,280 --> 00:41:34,280
好像以前有一套是AI的

896
00:41:34,280 --> 00:41:36,280
有一套是自己的AI

897
00:41:36,280 --> 00:41:39,280
那套好像也有同類型的故事

898
00:41:39,280 --> 00:41:42,280
那個男孩是Hurt dealing

899
00:41:42,280 --> 00:41:45,280
也是認同的

900
00:41:45,280 --> 00:41:48,280
究竟你做的那個

901
00:41:48,280 --> 00:41:50,280
是不是有一個

902
00:41:50,280 --> 00:41:54,280
你會不會傷害到他

903
00:41:54,280 --> 00:41:57,280
什麼叫做你做AI出來

904
00:41:57,280 --> 00:41:59,280
是你傷害到他

905
00:41:59,280 --> 00:42:01,280
因為你傷害到他

906
00:42:01,280 --> 00:42:06,280
其實是假設了他有一種自我意識和感受

907
00:42:06,280 --> 00:42:11,280
當你說其實現在我們的看法

908
00:42:11,280 --> 00:42:14,280
是那些不是有自我意識和感受

909
00:42:14,280 --> 00:42:16,280
那就不會了

910
00:42:16,280 --> 00:42:18,280
但複雜的地方是

911
00:42:18,280 --> 00:42:20,280
你怎麼知道他沒有呢?

912
00:42:20,280 --> 00:42:23,280
去到那個位置就很難回答了

913
00:42:23,280 --> 00:42:28,280
我就會從人自己本身內裡

914
00:42:28,280 --> 00:42:31,280
發生什麼事的看法出現

915
00:42:31,280 --> 00:42:33,280
我們預備的時候

916
00:42:33,280 --> 00:42:35,280
聊天的時候

917
00:42:35,280 --> 00:42:37,280
都在問一個問題

918
00:42:37,280 --> 00:42:39,280
因為上帝做人

919
00:42:39,280 --> 00:42:41,280
人就做AI

920
00:42:41,280 --> 00:42:45,280
剛才阮長華說按照自己的形象去做

921
00:42:45,280 --> 00:42:47,280
上帝做人

922
00:42:47,280 --> 00:42:49,280
他是會為人而死

923
00:42:49,280 --> 00:42:51,280
耶穌基督為我們而死

924
00:42:51,280 --> 00:42:53,280
人做AI

925
00:42:53,280 --> 00:42:55,280
你會不會為AI而死

926
00:42:55,280 --> 00:42:58,280
這是一個對人的挑戰

927
00:42:58,280 --> 00:43:00,280
去到某一天你會為AI而死

928
00:43:00,280 --> 00:43:04,280
你就跟他有一種關係在裡面

929
00:43:04,280 --> 00:43:06,280
我未必能回答這個問題

930
00:43:06,280 --> 00:43:11,280
但是人自己本身那種道德責任

931
00:43:11,280 --> 00:43:13,280
回到最後都要問自己

932
00:43:13,280 --> 00:43:15,280
我視這件事是什麼

933
00:43:15,280 --> 00:43:20,280
是一個工具還是我跟他有一個愛的關係

934
00:43:20,280 --> 00:43:22,280
會不會去到一個這樣的地步

935
00:43:22,280 --> 00:43:26,280
這是一個要繼續問下去的問題

936
00:43:26,280 --> 00:43:28,280
我不知道小蕾老師有沒有什麼回應

937
00:43:28,280 --> 00:43:32,280
沒有,我只想Westworld的電視劇

938
00:43:32,280 --> 00:43:35,280
終於有一個人想問問題

939
00:43:35,280 --> 00:43:37,280
你會是第一個拿咪問

940
00:43:37,280 --> 00:43:39,280
也是最後一個問問題

941
00:43:39,280 --> 00:43:41,280
時間到了

942
00:43:41,280 --> 00:43:49,280
多謝三位講員的分享

943
00:43:49,280 --> 00:43:53,280
都在豐富我們對於新世界的想像

944
00:43:53,280 --> 00:43:57,280
不過我也想問一個問題

945
00:43:57,280 --> 00:43:59,280
可能也切身關注

946
00:43:59,280 --> 00:44:03,280
當AI科技一直發展下去的時候

947
00:44:03,280 --> 00:44:08,280
其實不只是我們普通人用

948
00:44:08,280 --> 00:44:13,280
其實牽涉到商界甚至是政權的使用

949
00:44:13,280 --> 00:44:16,280
剛才說到一件事

950
00:44:16,280 --> 00:44:18,280
舉一個例子

951
00:44:18,280 --> 00:44:21,280
可能在要用一些科技的時候

952
00:44:21,280 --> 00:44:23,280
有些老人家用不了

953
00:44:23,280 --> 00:44:27,280
但是這個科技是用來監控人的

954
00:44:27,280 --> 00:44:31,280
是用來塑造人的本性、本質

955
00:44:31,280 --> 00:44:37,280
作為教會究竟是要達到這個要求

956
00:44:37,280 --> 00:44:41,280
去做這件事還是維持穩定

957
00:44:41,280 --> 00:44:44,280
因為這件事可能是幫助人活得更加舒服

958
00:44:44,280 --> 00:44:46,280
我們正在進入這個世界

959
00:44:46,280 --> 00:44:48,280
我們說要幫助人進入世界

960
00:44:48,280 --> 00:44:50,280
但同一時間我們又要抗拒這件事

961
00:44:50,280 --> 00:44:53,280
究竟教會應該怎樣回應這件事呢?

962
00:44:53,280 --> 00:44:56,280
怎樣回應使用科技上呢?

963
00:44:56,280 --> 00:44:58,280
我想這是幾方面的問題

964
00:44:58,280 --> 00:45:00,280
多謝

965
00:45:00,280 --> 00:45:07,280
我先試一下回答

966
00:45:07,280 --> 00:45:11,280
剛才你說得很好

967
00:45:11,280 --> 00:45:16,280
一方面我們在整個現代社會

968
00:45:16,280 --> 00:45:21,280
你不進入科技的社會

969
00:45:21,280 --> 00:45:24,280
你就會被淘汰

970
00:45:24,280 --> 00:45:26,280
差不多是必然地

971
00:45:26,280 --> 00:45:30,280
你不玩了,去深山自己住

972
00:45:30,280 --> 00:45:36,280
不然你就一定要去學或做科技的東西

973
00:45:36,280 --> 00:45:38,280
你才不會被淘汰

974
00:45:38,280 --> 00:45:40,280
舉個例子

975
00:45:40,280 --> 00:45:42,280
你用查GPT

976
00:45:42,280 --> 00:45:45,280
你當所有其他同學都懂得用查GPT去找答案

977
00:45:45,280 --> 00:45:48,280
但你不懂的,你當然會慢很多

978
00:45:48,280 --> 00:45:51,280
然後你就會慢慢被淘汰

979
00:45:51,280 --> 00:45:53,280
如果這是一個遊戲規則

980
00:45:53,280 --> 00:45:58,280
然後,我忘記了之後的問題

981
00:45:58,280 --> 00:46:04,280
如果沒有科技,那怎樣?

982
00:46:05,280 --> 00:46:09,280
即使是雙人,同一系統的科技

983
00:46:09,280 --> 00:46:13,280
你包括一些職業科技的人

984
00:46:13,280 --> 00:46:16,280
這是一個牧師的問題

985
00:46:16,280 --> 00:46:18,280
多謝

986
00:46:18,280 --> 00:46:24,280
很難回答

987
00:46:24,280 --> 00:46:29,280
因為問題的弟兄其實是說了兩者

988
00:46:29,280 --> 00:46:34,280
都不會有一個絕對偏一邊

989
00:46:34,280 --> 00:46:40,280
我會說一個甚麼的案件

990
00:46:40,280 --> 00:46:44,280
很難以片蓋全地說

991
00:46:44,280 --> 00:46:47,280
你可以怎樣

992
00:46:47,280 --> 00:46:54,280
當然我們不應該去幫Totalitarianism

993
00:46:54,280 --> 00:46:58,280
但其實甚麼才是Totalitarianism呢?

994
00:46:58,280 --> 00:47:03,280
但你初時Google Map幫你開車

995
00:47:03,280 --> 00:47:05,280
是不是Totalitarianism呢?

996
00:47:05,280 --> 00:47:07,280
你明白嗎?

997
00:47:07,280 --> 00:47:09,280
誰不用呢?

998
00:47:09,280 --> 00:47:11,280
很方便

999
00:47:11,280 --> 00:47:13,280
老鬼不喜歡用

1000
00:47:13,280 --> 00:47:15,280
看了地圖,但現在地圖很難買

1001
00:47:15,280 --> 00:47:21,280
你明白嗎?其實你都要收很多資料

1002
00:47:21,280 --> 00:47:25,280
但原初發展出來,其實真的很有用

1003
00:47:25,280 --> 00:47:29,280
你跟著說你給了誰不良資料去用

1004
00:47:29,280 --> 00:47:31,280
但又真的可以

1005
00:47:31,280 --> 00:47:33,280
那怎樣呢?

1006
00:47:33,280 --> 00:47:36,280
其實是很複雜的

1007
00:47:36,280 --> 00:47:40,280
如果真的要這樣回應

1008
00:47:40,280 --> 00:47:46,280
是要看一個很實質的,究竟在說甚麼事

1009
00:47:46,280 --> 00:47:48,280
發生了甚麼事

1010
00:47:48,280 --> 00:47:52,280
在裏面可能做到某些決定

1011
00:47:52,280 --> 00:47:57,280
最後一個回應是比較好,因為有很多時間去想

1012
00:47:57,280 --> 00:48:01,280
我會看教會是甚麼呢?

1013
00:48:01,280 --> 00:48:06,280
教會不只是堂會,不是說牧師怎樣回應

1014
00:48:06,280 --> 00:48:11,280
而是教會是說整個基督徒群體如何去回應

1015
00:48:11,280 --> 00:48:13,280
如果這樣角度去看

1016
00:48:13,280 --> 00:48:18,280
你會發現每一個個體在不同的崗位裏

1017
00:48:18,280 --> 00:48:22,280
每一個人都可以有自己在崗位裏的回應

1018
00:48:22,280 --> 00:48:24,280
如果你是一個牧師

1019
00:48:24,280 --> 00:48:30,280
你會去鼓勵信徒,回應信徒的品格能力

1020
00:48:30,280 --> 00:48:32,280
去回應他自己用科技

1021
00:48:32,280 --> 00:48:35,280
如果你是一個基督徒

1022
00:48:35,280 --> 00:48:42,280
你是在做管理,或者制度上的事

1023
00:48:42,280 --> 00:48:45,280
你就要問自己,在那一刻裏

1024
00:48:45,280 --> 00:48:50,280
聖經對你來說,你要做一些規範和政策的時候

1025
00:48:50,280 --> 00:48:52,280
你是一個怎樣的人呢?

1026
00:48:52,280 --> 00:48:55,280
當然,如果你是一個技術很強的人

1027
00:48:55,280 --> 00:48:59,280
你可以幫助教會去使用這些科技

1028
00:48:59,280 --> 00:49:04,280
以至無論宣教,教會的運作,幫助一些困難的人等等

1029
00:49:04,280 --> 00:49:08,280
每一個人都應該有自己的角色在裏面

1030
00:49:08,280 --> 00:49:10,280
這才是教會的正題

1031
00:49:10,280 --> 00:49:13,280
宗神說的是collaboration(合作)

1032
00:49:13,280 --> 00:49:17,280
所以那一刻,我們怎樣可以從一個整體的角度

1033
00:49:17,280 --> 00:49:22,280
去回應這麼大的挑戰

1034
00:49:22,280 --> 00:49:26,280
這是一個共同的研習課題

1035
00:49:26,280 --> 00:49:32,280
不知道宗神的同學,遲些會做一個capstone的論文

1036
00:49:32,280 --> 00:49:34,280
去講一下就最好了

1037
00:49:34,280 --> 00:49:42,280
多謝Alex用每個人要做反思協作者去回應這個議題的定義

1038
00:49:42,280 --> 00:49:45,280
再次多謝三位講員

1039
00:49:45,280 --> 00:49:53,280
最後請院長做一個結束的禱告

1040
00:49:53,280 --> 00:49:55,280
不如我們一起祈禱

1041
00:49:55,280 --> 00:50:06,280
天父,我們知道你創造我們

1042
00:50:06,280 --> 00:50:09,280
你將我們放在時代裏面

1043
00:50:09,280 --> 00:50:12,280
亦都出現AI這樣的事情

1044
00:50:12,280 --> 00:50:15,280
天父,我們見到AI都是你給我們

1045
00:50:15,280 --> 00:50:19,280
能夠去彰顯到人類的智慧

1046
00:50:19,280 --> 00:50:23,280
又或者在很多地方都能夠幫助我們做很多事情

1047
00:50:23,280 --> 00:50:26,280
不過同時我們都見到AI給我們很多的影響

1048
00:50:26,280 --> 00:50:28,280
我們都要去到小心

1049
00:50:28,280 --> 00:50:31,280
天父,首先我們來到你的面前

1050
00:50:31,280 --> 00:50:34,280
我們都要承認,都要謙虛地去到學習

1051
00:50:34,280 --> 00:50:39,280
讓我們都真是懂得如何辨識今天的世代

1052
00:50:39,280 --> 00:50:42,280
在今天的世代裏面作你忠心的僕人

1053
00:50:42,280 --> 00:50:45,280
多謝你給我們今晚一些相聚的時間

1054
00:50:45,280 --> 00:50:49,280
我們不敢說有很深的道理能夠出來

1055
00:50:49,280 --> 00:50:53,280
不過我們希望今天的訊息能夠幫助到弟兄姊妹

1056
00:50:53,280 --> 00:50:56,280
我們認真面對這個課題

1057
00:50:56,280 --> 00:50:58,280
懂得開始去思考

1058
00:50:58,280 --> 00:51:02,280
亦都懂得去留意AI給我們的影響

1059
00:51:02,280 --> 00:51:06,280
求你繼續幫助我們往後的日子

1060
00:51:06,280 --> 00:51:10,280
我們有空間有機會更加多去思想

1061
00:51:10,280 --> 00:51:14,280
亦都更加多去善用你給我們的恩賜

1062
00:51:14,280 --> 00:51:18,280
亦都小心這個世界給我們的引誘和陷阱

1063
00:51:18,280 --> 00:51:20,280
所以求你這樣聽我們的祈禱

1064
00:51:20,280 --> 00:51:22,280
祈禱供奉耶穌,記得我們的名字祈求,阿門

1065
00:51:22,280 --> 00:51:25,280
多謝大家,今天就到此為止

1066
00:51:26,280 --> 00:51:37,280
(音樂)

